{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"2AiEquVKc481","colab_type":"code","outputId":"8075725d-3bf4-40bc-9a7f-e3183b11f5b4","executionInfo":{"status":"ok","timestamp":1588695023727,"user_tz":-120,"elapsed":13187,"user":{"displayName":"Xavier López","photoUrl":"","userId":"17793741074420892205"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!pip install wandb"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting wandb\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/c9/ebbcefa6ef2ba14a7c62a4ee4415a5fecef8fac5e4d1b4e22af26fd9fe22/wandb-0.8.35-py2.py3-none-any.whl (1.4MB)\n","\u001b[K     |████████████████████████████████| 1.4MB 2.9MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.23.0)\n","Collecting shortuuid>=0.5.0\n","  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n","Collecting configparser>=3.8.1\n","  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n","Collecting subprocess32>=3.5.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n","\u001b[K     |████████████████████████████████| 102kB 9.4MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n","Collecting sentry-sdk>=0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/7e/19545324e83db4522b885808cd913c3b93ecc0c88b03e037b78c6a417fa8/sentry_sdk-0.14.3-py2.py3-none-any.whl (103kB)\n","\u001b[K     |████████████████████████████████| 112kB 17.8MB/s \n","\u001b[?25hCollecting docker-pycreds>=0.4.0\n","  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n","Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n","Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n","Collecting gql==0.2.0\n","  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n","Collecting watchdog>=0.8.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/c3/ed6d992006837e011baca89476a4bbffb0a91602432f73bd4473816c76e2/watchdog-0.10.2.tar.gz (95kB)\n","\u001b[K     |████████████████████████████████| 102kB 9.1MB/s \n","\u001b[?25hCollecting GitPython>=1.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/33/917e6fde1cad13daa7053f39b7c8af3be287314f75f1b1ea8d3fe37a8571/GitPython-3.1.2-py3-none-any.whl (451kB)\n","\u001b[K     |████████████████████████████████| 460kB 17.0MB/s \n","\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2020.4.5.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n","Collecting graphql-core<2,>=0.5.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n","\u001b[K     |████████████████████████████████| 71kB 8.1MB/s \n","\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n","Collecting pathtools>=0.1.1\n","  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n","Collecting gitdb<5,>=4.0.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n","\u001b[K     |████████████████████████████████| 71kB 8.8MB/s \n","\u001b[?25hCollecting smmap<4,>=3.0.1\n","  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n","Building wheels for collected packages: subprocess32, gql, watchdog, graphql-core, pathtools\n","  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=b09312b9ffa738f6e4a4e0945b7b0cec4dfa353a40d49ba7fc69565a910a060d\n","  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n","  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=5d778c306eb9e8e82e4bbd7c9cd6a1fbdaad87e6a26d0be94da07a3bc913a843\n","  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n","  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for watchdog: filename=watchdog-0.10.2-cp36-none-any.whl size=73605 sha256=93dac565d97e3a094c5c9cb24124109a254ad408f4a5867c249439257068c2b9\n","  Stored in directory: /root/.cache/pip/wheels/bc/ed/6c/028dea90d31b359cd2a7c8b0da4db80e41d24a59614154072e\n","  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104650 sha256=ba9db76f04a971a94efe5849d215dd2f6c161a022754b47804515f93abda8862\n","  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8784 sha256=43fe5269500ec5dccd427a861036c81db66ebf8dec55804c4717ef6218d093ae\n","  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n","Successfully built subprocess32 gql watchdog graphql-core pathtools\n","Installing collected packages: shortuuid, configparser, subprocess32, sentry-sdk, docker-pycreds, graphql-core, gql, pathtools, watchdog, smmap, gitdb, GitPython, wandb\n","Successfully installed GitPython-3.1.2 configparser-5.0.0 docker-pycreds-0.4.0 gitdb-4.0.5 gql-0.2.0 graphql-core-1.1 pathtools-0.1.2 sentry-sdk-0.14.3 shortuuid-1.0.1 smmap-3.0.4 subprocess32-3.5.4 wandb-0.8.35 watchdog-0.10.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"88ezc8iRdkqg","colab_type":"code","outputId":"156ef69f-c51a-4d7d-c3ae-124557bfc967","executionInfo":{"status":"ok","timestamp":1588695341829,"user_tz":-120,"elapsed":12010,"user":{"displayName":"Xavier López","photoUrl":"","userId":"17793741074420892205"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["!wandb login"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://app.wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: dff003aa03e7d25df35a840b6f0660ae9675efb4\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8skRpihGdp1q","colab_type":"code","outputId":"adf78af9-35ea-4768-fece-2eedfbe623b2","executionInfo":{"status":"ok","timestamp":1588695343296,"user_tz":-120,"elapsed":13142,"user":{"displayName":"Xavier López","photoUrl":"","userId":"17793741074420892205"}},"colab":{"base_uri":"https://localhost:8080/","height":307}},"source":["#GPU INFO\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Tue May  5 16:15:47 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   39C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"csakjx4mdrs5","colab_type":"code","outputId":"20cff465-e726-4f56-9f2d-4315e510d8a9","executionInfo":{"status":"ok","timestamp":1588695343297,"user_tz":-120,"elapsed":12677,"user":{"displayName":"Xavier López","photoUrl":"","userId":"17793741074420892205"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zoV2tev1jHbQ","colab_type":"code","colab":{}},"source":["import sys\n","sys.path.append('/content/drive/My Drive/Colab_Notebooks/git/PFM_Noisy_Labels/MLNT_cifar')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xq88YZuGhRTA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"af792cfb-83b6-4fcd-d739-3c36f5c7a488","executionInfo":{"status":"ok","timestamp":1588695355853,"user_tz":-120,"elapsed":10895,"user":{"displayName":"Xavier López","photoUrl":"","userId":"17793741074420892205"}}},"source":["# Pytorch libraries\n","import torch\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","\n","# Internal files\n","import config\n","import dataloader\n","import models\n","# from baseline import get_model, save_checkpoint\n","\n","import math\n","import os\n","import sys\n","import time\n","from collections import OrderedDict\n","import random\n","\n","import wandb\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["\n","noise file noisy_label_kv.txt generated with noise: 0.1\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n","                Project page: <a href=\"https://app.wandb.ai/xavierlopeze/Cifar_Experiment\" target=\"_blank\">https://app.wandb.ai/xavierlopeze/Cifar_Experiment</a><br/>\n","                Run page: <a href=\"https://app.wandb.ai/xavierlopeze/Cifar_Experiment/runs/3rfi9qze\" target=\"_blank\">https://app.wandb.ai/xavierlopeze/Cifar_Experiment/runs/3rfi9qze</a><br/>\n","            "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"tsMIDod2mpPz","colab_type":"code","colab":{}},"source":["def get_model():\n","\n","    # Get model from config\n","    if config.model == \"resnet18\":\n","        model = models.resnet18(pretrained=config.pretrained)\n","    elif config.model == \"resnet34\":\n","        model = models.resnet34(pretrained=config.pretrained)\n","    elif config.model == 'resnet50':\n","        model = models.resnet50(pretrained=config.pretrained)\n","    elif config.model == \"resnet101\":\n","        model = models.resnet101(pretrained=config.pretrained)\n","    elif config.model == \"resnet152\":\n","        model = models.resnet152(pretrained=config.pretrained)\n","    elif config.model == \"resnext50_32x4d\":\n","        model = models.resnet34(pretrained=config.pretrained)\n","    elif config.model == 'resnext101_32x8d':\n","        model = models.resnet50(pretrained=config.pretrained)\n","    elif config.model == \"wide_resnet50_2\":\n","        model = models.resnet101(pretrained=config.pretrained)\n","    elif config.model == \"wide_resnet101_2\":\n","        model = models.resnet152(pretrained=config.pretrained)\n","    else:\n","        raise ValueError('%s not supported'.format(config.model))\n","\n","    # Initialize fc layer\n","    (in_features, out_features) = model.fc.in_features, model.fc.out_features\n","    model.fc = torch.nn.Linear(in_features, out_features)\n","    return model\n","\n","\n","\n","def save_checkpoint(state, filename='checkpoint.pth.tar'):\n","    torch.save(state, filename)\n","    if config.use_wandb == True:\n","        wandb.save(filename)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UG1lpyQugVk3","colab_type":"code","colab":{}},"source":["def scheduler(epoch: int):\n","    global lr\n","    lr = config.lr\n","    if epoch > config.start_epoch:\n","        lr = lr / 10.0\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","# Training\n","def train(epoch):\n","    global init\n","    net.train()\n","    tch_net.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    scheduler(epoch)\n","\n","\n","    # ramp up meta-learning rate and EMA decay\n","    if epoch <= config.param_epoch:\n","        u = epoch/config.param_epoch\n","        meta_lr = config.meta_lr*math.exp(-5*(1-u)**2)\n","    else:\n","        meta_lr = config.meta_lr\n","        config.eps = 0.999\n","\n","    for step, (inputs, targets) in enumerate(train_loader):\n","        init_time = time.time()\n","        if use_cuda:\n","            inputs, targets = inputs.cuda(), targets.cuda()\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","\n","        class_loss = criterion(outputs, targets)\n","        class_loss.backward(retain_graph=True)\n","        \n","\n","        if step > config.start_iter or epoch > 1:\n","        #if step > 0 or epoch > 0:\n","\n","            # if step > config.mid_iter or epoch > 1:\n","            #     # config.eps = 0.999\n","            #     alpha = config.alpha \n","            # else:\n","            #     u = (step - config.start_iter)/(config.mid_iter - config.start_iter)\n","            #     alpha = config.alpha*math.exp(-5*(1-u)**2)\n","            alpha = config.alpha\n","\n","            if init:\n","                init = False\n","                for param, param_tch in zip(net.parameters(), tch_net.parameters()):\n","                    param_tch.data.copy_(param.data)\n","            else:\n","                for param, param_tch in zip(net.parameters(), tch_net.parameters()):\n","                    param_tch.data.mul_(config.eps).add_((1-config.eps), param.data)\n","\n","            _, feats = pretrain_net(inputs, get_feat=True)\n","            tch_outputs = tch_net(inputs, get_feat=False)\n","            p_tch = F.softmax(tch_outputs, dim=1)\n","            p_tch.detach_()\n","\n","            for i in range(config.num_fast):\n","                targets_fast = targets.clone()\n","                randidx = torch.randperm(targets.size(0))\n","                for n in range(int(targets.size(0)*config.perturb_ratio)):\n","                    num_neighbor = 10\n","                    idx = randidx[n]\n","                    feat = feats[idx]\n","                    feat.view(1, feat.size(0))\n","                    feat.data = feat.data.expand(targets.size(0), feat.size(0))\n","                    dist = torch.sum((feat-feats)**2, dim=1)\n","                    _, neighbor = torch.topk(dist.data, num_neighbor+1, largest=False)\n","                    targets_fast[idx] = targets[neighbor[random.randint(1, num_neighbor)]]\n","\n","                fast_loss = criterion(outputs, targets_fast)\n","\n","                grads = torch.autograd.grad(fast_loss, net.parameters(),\n","                                            create_graph=False,\n","                                            retain_graph=True,\n","                                            only_inputs=True)\n","\n","                fast_weights = OrderedDict(\n","                    (name, param - meta_lr*grad)\n","                    for ((name, param), grad) in zip(net.named_parameters(), grads))\n","\n","                fast_out = net.forward(inputs,fast_weights)\n","\n","                logp_fast = F.log_softmax(fast_out,dim=1)\n","                #afegir canvis per iterative aquí\n","                consistent_loss = consistent_criterion(logp_fast, p_tch)\n","                consistent_loss = consistent_loss*alpha/config.num_fast\n","                consistent_loss.backward()\n","\n","        optimizer.step()\n","\n","        # train_loss += class_loss.data.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets.data).cpu().sum()\n","\n","        # Grab training results\n","        sys.stdout.write('\\r')\n","        sys.stdout.write('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%, time: %.3f'\n","              %(epoch, config.num_epochs, step+1, (len(train_loader.dataset)//config.batch_size)+1, class_loss.data.item(), 100.*correct/total,time.time() - init_time))\n","        sys.stdout.flush()\n","\n","\n","\n","def valid(epoch, network):\n","    global best_acc\n","    network.eval()\n","    # val_loss = 0\n","    correct = 0\n","    total = 0\n","    for step, (inputs, targets) in enumerate(valid_loader):\n","        if use_cuda:\n","            inputs, targets = inputs.cuda(), targets.cuda()\n","        with torch.no_grad():\n","            outputs = network(inputs)\n","            loss = criterion(outputs, targets)\n","\n","        # valid_loss += loss.data.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets.data).cpu().sum()\n","\n","        # Grab validation results\n","        valid_acc = 100. * correct / total\n","      # valid_results = (\"| Epoch: {}/{}, val_loss: {:.3f}, val_acc: {:.3f}, \"\n","      #                 \"lr: {:.6f}\".format(epoch,\n","      #                                     config.num_epochs,\n","      #                                     loss.data.item(),\n","      #                                     valid_acc,\n","      #                                     lr))\n","        # Grab validation results\n","        valid_results = (\"| Epoch: {}/{}, val_loss: {:.3f}, val_acc: {:.3f}, \"\"lr: {:.6f}\".format(epoch,config.num_epochs,loss.data.item(),valid_acc,lr))\n","        record.write(valid_results + '\\n')\n","        record.flush()\n","       \n","        \n","\n","    # Save checkpoint when best model\n","    if valid_acc > best_acc:\n","        best_acc = valid_acc\n","        print('| Saving Best Model ...', end=\"\\r\")\n","        save_point = config.drive_dir + '/checkpoint/' + config.id + '.pth.tar' \n","        save_checkpoint({\n","            'state_dict': network.state_dict(),\n","            'best_acc': best_acc,\n","        }, save_point)\n","     \n","    wandb.log({'epoch': epoch, 'accy_val' : best_acc })\n","\n","    return valid_results\n","\n","\n","def test():\n","    test_net.eval()\n","    # test_loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (inputs, targets) in enumerate(valid_loader):\n","        if use_cuda:\n","            inputs, targets = inputs.cuda(), targets.cuda()\n","        with torch.no_grad():\n","            outputs = test_net(inputs)\n","            loss = criterion(outputs, targets)\n","\n","        # test_loss += loss.data.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets.data).cpu().sum()\n","\n","    # Grab validation results\n","    test_acc = 100. * correct/total\n","    test_results = \"| test_loss: {:.3f}, test_acc: {:.3f}\".format(\n","        loss.data.item(), test_acc)\n","    record.write(test_results)\n","    record.flush()\n","\n","    print(test_results)\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NEBEekTkLEP4","colab_type":"code","colab":{}},"source":["def save_weights(epoch):\n","        print('| Saving Weights student ...', end=\"\\r\")\n","        save_point = config.drive_dir + '/checkpoint/' + config.id + '_student_' + str(epoch) + '.pth.tar'\n","        save_checkpoint({'state_dict': net.state_dict(), }, save_point)\n","\n","        print('| Saving Weights teacher ...', end=\"\\r\")\n","        save_point = config.drive_dir + '/checkpoint/' + config.id + '_teacher_' + str(epoch) + '.pth.tar'\n","        save_checkpoint({'state_dict': tch_net.state_dict(), }, save_point)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9mjWKzpX9L_W","outputId":"a4e4a361-0927-4079-8030-169ba665b655","executionInfo":{"status":"ok","timestamp":1588695379860,"user_tz":-120,"elapsed":15230,"user":{"displayName":"Xavier López","photoUrl":"","userId":"17793741074420892205"}},"colab":{"base_uri":"https://localhost:8080/","height":273}},"source":["# Checkpoint dir.\n","record = open(config.drive_dir + '/checkpoint/' + config.checkpoint + '_test.txt', 'w')\n","record.write('noise_rate=%s\\n' % config.noise_rate)\n","record.flush()\n","\n","# Get the original_dataset\n","loader = dataloader.KeyDataLoader()\n","train_loader, valid_loader, test_loader = loader.run()\n","\n","# Hyper Parameter settings\n","random.seed(config.seed)\n","# torch.cuda.set_device(config.gpuid)\n","torch.manual_seed(config.seed)\n","torch.cuda.manual_seed_all(config.seed)\n","use_cuda = torch.cuda.is_available()\n","\n","# Networks setup\n","print('\\nModel setup')\n","print('| Building network: {}'.format(config.model))\n","net = get_model()\n","tch_net = get_model()\n","pretrain_net = get_model()\n","test_net = get_model()\n","\n","print('| load pretrained net. from checkpoint...')\n","checkpoint = torch.load(config.drive_dir + '/checkpoint/' + config.checkpoint + '.pth.tar')\n","pretrain_net.load_state_dict(checkpoint['state_dict'])\n","\n","if use_cuda:\n","    net.cuda()\n","    tch_net.cuda()\n","    pretrain_net.cuda()\n","    test_net.cuda()\n","    cudnn.benchmark = True\n","pretrain_net.eval()\n","\n","for param in tch_net.parameters():\n","    param.requires_grad = False\n","for param in pretrain_net.parameters():\n","    param.requires_grad = False\n","\n","# Instantiate a loss function.\n","criterion = torch.nn.CrossEntropyLoss()\n","consistent_criterion = torch.nn.KLDivLoss()\n","\n","# Instantiate an optimizer to train the model\n","optimizer = torch.optim.SGD(\n","    net.parameters(), lr=config.lr, momentum=config.momentum, weight_decay=config.weight_decay)\n","\n","print('\\nTraining model')\n","print('| Training Epochs = ' + str(config.num_epochs))\n","print('| Initial Learning Rate = ' + str(config.lr))\n","print('| Optimizer = ' + str(config.optimizer_type))\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["\n","Model setup\n","| Building network: resnet34\n"],"name":"stdout"},{"output_type":"stream","text":["/content/drive/My Drive/Colab_Notebooks/git/PFM_Noisy_Labels/MLNT_cifar/models/resnet.py:122: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n","  nn.init.kaiming_normal(m.weight, mode='fan_out')\n","/content/drive/My Drive/Colab_Notebooks/git/PFM_Noisy_Labels/MLNT_cifar/models/resnet.py:124: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","  nn.init.constant(m.weight, 1)\n","/content/drive/My Drive/Colab_Notebooks/git/PFM_Noisy_Labels/MLNT_cifar/models/resnet.py:125: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","  nn.init.constant(m.bias, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["| load pretrained net. from checkpoint...\n","\n","Training model\n","| Training Epochs = 120\n","| Initial Learning Rate = 0.2\n","| Optimizer = SGD\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bZRHtbXGmqnM","colab_type":"code","outputId":"e30b110f-20c4-40f7-d257-cc2b97c297dc","executionInfo":{"status":"error","timestamp":1588606789592,"user_tz":-120,"elapsed":181125,"user":{"displayName":"Xavier López","photoUrl":"","userId":"17793741074420892205"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["init = True\n","best_acc = 0\n","for epoch in range(1, 1 + config.num_epochs):\n","    train(epoch)\n","    # Student validation\n","    std_results = valid(epoch, net)\n","    record.write(std_results + '\\n')\n","    print(std_results)\n","    # Teacher validation\n","    tch_results = valid(epoch, tch_net)\n","    record.write(tch_results + '\\n')\n","    record.flush()\n","    print(tch_results)\n","\n","    save_weights(epoch)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["| Epoch: 1/120, val_loss: 2.159, val_acc: 22.400, lr: 0.200000\n","| Epoch: 1/120, val_loss: 23.725, val_acc: 0.000, lr: 0.200000\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2247: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"],"name":"stderr"},{"output_type":"stream","text":["\r| Epoch [  2/120] Iter[  1/352]\t\tLoss: 2.2991 Acc@1: 14.844%, time: 3.439"],"name":"stdout"},{"output_type":"stream","text":["/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha)\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch: 2/120, val_loss: 1.494, val_acc: 25.660, lr: 0.200000\n","| Epoch: 2/120, val_loss: 1.761, val_acc: 33.620, lr: 0.200000\n","| Epoch [  3/120] Iter[352/352]\t\tLoss: 1.6831 Acc@1: 31.298%, time: 1.228| Epoch: 3/120, val_loss: 2.261, val_acc: 29.520, lr: 0.200000\n","| Epoch: 3/120, val_loss: 1.703, val_acc: 41.020, lr: 0.200000\n","| Epoch [  4/120] Iter[352/352]\t\tLoss: 1.5851 Acc@1: 37.660%, time: 1.217| Epoch: 4/120, val_loss: 1.955, val_acc: 39.060, lr: 0.200000\n","| Epoch: 4/120, val_loss: 1.271, val_acc: 47.980, lr: 0.200000\n","| Epoch [  5/120] Iter[352/352]\t\tLoss: 1.8387 Acc@1: 42.727%, time: 1.223| Epoch: 5/120, val_loss: 2.051, val_acc: 44.320, lr: 0.200000\n","| Epoch: 5/120, val_loss: 0.925, val_acc: 54.040, lr: 0.200000\n","| Epoch [  6/120] Iter[352/352]\t\tLoss: 1.5219 Acc@1: 47.873%, time: 1.216| Epoch: 6/120, val_loss: 1.451, val_acc: 49.700, lr: 0.200000\n","| Epoch: 6/120, val_loss: 0.585, val_acc: 60.460, lr: 0.200000\n","| Epoch [  7/120] Iter[352/352]\t\tLoss: 1.1587 Acc@1: 52.938%, time: 1.211| Epoch: 7/120, val_loss: 0.798, val_acc: 60.240, lr: 0.200000\n","| Epoch: 7/120, val_loss: 0.477, val_acc: 66.740, lr: 0.200000\n","| Epoch [  8/120] Iter[352/352]\t\tLoss: 1.2721 Acc@1: 57.476%, time: 1.223| Epoch: 8/120, val_loss: 0.874, val_acc: 63.400, lr: 0.200000\n","| Epoch: 8/120, val_loss: 0.414, val_acc: 71.200, lr: 0.200000\n","| Epoch [  9/120] Iter[352/352]\t\tLoss: 1.2633 Acc@1: 60.916%, time: 1.224| Epoch: 9/120, val_loss: 0.267, val_acc: 61.760, lr: 0.200000\n","| Epoch: 9/120, val_loss: 0.258, val_acc: 74.560, lr: 0.200000\n","| Epoch [ 10/120] Iter[352/352]\t\tLoss: 1.3029 Acc@1: 63.280%, time: 1.225| Epoch: 10/120, val_loss: 0.283, val_acc: 71.160, lr: 0.200000\n","| Epoch: 10/120, val_loss: 0.383, val_acc: 77.340, lr: 0.200000\n","| Epoch [ 11/120] Iter[352/352]\t\tLoss: 1.1366 Acc@1: 66.020%, time: 1.218| Epoch: 11/120, val_loss: 0.504, val_acc: 67.560, lr: 0.200000\n","| Epoch: 11/120, val_loss: 0.387, val_acc: 79.960, lr: 0.200000\n","| Epoch [ 12/120] Iter[352/352]\t\tLoss: 1.0170 Acc@1: 68.611%, time: 1.215| Epoch: 12/120, val_loss: 0.750, val_acc: 67.100, lr: 0.200000\n","| Epoch: 12/120, val_loss: 0.465, val_acc: 81.600, lr: 0.200000\n","| Epoch [ 13/120] Iter[352/352]\t\tLoss: 1.1338 Acc@1: 70.553%, time: 1.225| Epoch: 13/120, val_loss: 0.852, val_acc: 65.000, lr: 0.200000\n","| Epoch: 13/120, val_loss: 0.394, val_acc: 83.300, lr: 0.200000\n","| Epoch [ 14/120] Iter[352/352]\t\tLoss: 0.9499 Acc@1: 71.922%, time: 1.223| Epoch: 14/120, val_loss: 0.746, val_acc: 66.700, lr: 0.200000\n","| Epoch: 14/120, val_loss: 0.435, val_acc: 84.720, lr: 0.200000\n","| Epoch [ 15/120] Iter[352/352]\t\tLoss: 0.8328 Acc@1: 72.944%, time: 1.228| Epoch: 15/120, val_loss: 0.906, val_acc: 69.080, lr: 0.200000\n","| Epoch: 15/120, val_loss: 0.573, val_acc: 85.540, lr: 0.200000\n","| Epoch [ 16/120] Iter[352/352]\t\tLoss: 1.1444 Acc@1: 74.036%, time: 1.215| Epoch: 16/120, val_loss: 0.871, val_acc: 68.020, lr: 0.200000\n","| Epoch: 16/120, val_loss: 0.635, val_acc: 86.340, lr: 0.200000\n","| Epoch [ 17/120] Iter[352/352]\t\tLoss: 0.8955 Acc@1: 74.787%, time: 1.222| Epoch: 17/120, val_loss: 1.141, val_acc: 66.940, lr: 0.200000\n","| Epoch: 17/120, val_loss: 0.475, val_acc: 86.820, lr: 0.200000\n","| Epoch [ 18/120] Iter[352/352]\t\tLoss: 0.9875 Acc@1: 75.504%, time: 1.215| Epoch: 18/120, val_loss: 0.303, val_acc: 68.280, lr: 0.200000\n","| Epoch: 18/120, val_loss: 0.470, val_acc: 88.120, lr: 0.200000\n","| Epoch [ 19/120] Iter[352/352]\t\tLoss: 1.1380 Acc@1: 75.996%, time: 1.220| Epoch: 19/120, val_loss: 0.704, val_acc: 73.520, lr: 0.200000\n","| Epoch: 19/120, val_loss: 0.606, val_acc: 88.000, lr: 0.200000\n","| Epoch [ 20/120] Iter[352/352]\t\tLoss: 0.8633 Acc@1: 76.644%, time: 1.218| Epoch: 20/120, val_loss: 0.551, val_acc: 68.760, lr: 0.200000\n","| Epoch: 20/120, val_loss: 0.531, val_acc: 88.600, lr: 0.200000\n","| Epoch [ 21/120] Iter[352/352]\t\tLoss: 0.6664 Acc@1: 77.251%, time: 1.211| Epoch: 21/120, val_loss: 0.357, val_acc: 76.640, lr: 0.200000\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error uploading \"MLNT_r10_ResNet34_BasicBlock.pth.tar\": CommError, /tmp/tmpat5keti4wandb/MLNT_r10_ResNet34_BasicBlock.pth.tar is an empty file\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch: 21/120, val_loss: 0.508, val_acc: 88.940, lr: 0.200000\n","| Epoch [ 22/120] Iter[352/352]\t\tLoss: 1.0617 Acc@1: 77.780%, time: 1.214| Epoch: 22/120, val_loss: 0.804, val_acc: 66.260, lr: 0.200000\n","| Epoch: 22/120, val_loss: 0.474, val_acc: 89.340, lr: 0.200000\n","| Epoch [ 23/120] Iter[352/352]\t\tLoss: 0.8012 Acc@1: 78.060%, time: 1.220| Epoch: 23/120, val_loss: 0.565, val_acc: 76.280, lr: 0.200000\n","| Epoch: 23/120, val_loss: 0.531, val_acc: 89.780, lr: 0.200000\n","| Epoch [ 24/120] Iter[352/352]\t\tLoss: 0.6781 Acc@1: 78.267%, time: 1.227| Epoch: 24/120, val_loss: 0.638, val_acc: 80.080, lr: 0.200000\n","| Epoch: 24/120, val_loss: 0.528, val_acc: 89.940, lr: 0.200000\n","| Epoch [ 25/120] Iter[352/352]\t\tLoss: 0.8825 Acc@1: 78.684%, time: 1.235| Epoch: 25/120, val_loss: 0.673, val_acc: 79.640, lr: 0.200000\n","| Epoch: 25/120, val_loss: 0.528, val_acc: 90.000, lr: 0.200000\n","| Epoch [ 26/120] Iter[352/352]\t\tLoss: 0.9400 Acc@1: 78.818%, time: 1.232| Epoch: 26/120, val_loss: 0.603, val_acc: 75.260, lr: 0.200000\n","| Epoch: 26/120, val_loss: 0.470, val_acc: 90.360, lr: 0.200000\n","| Epoch [ 27/120] Iter[352/352]\t\tLoss: 0.7886 Acc@1: 79.649%, time: 1.231| Epoch: 27/120, val_loss: 0.231, val_acc: 70.600, lr: 0.200000\n","| Epoch: 27/120, val_loss: 0.484, val_acc: 90.740, lr: 0.200000\n","| Epoch [ 28/120] Iter[352/352]\t\tLoss: 1.0456 Acc@1: 79.462%, time: 1.207| Epoch: 28/120, val_loss: 1.149, val_acc: 73.520, lr: 0.200000\n","| Epoch: 28/120, val_loss: 0.377, val_acc: 90.980, lr: 0.200000\n","| Epoch [ 29/120] Iter[352/352]\t\tLoss: 0.7459 Acc@1: 79.862%, time: 1.215| Epoch: 29/120, val_loss: 0.767, val_acc: 71.400, lr: 0.200000\n","| Epoch: 29/120, val_loss: 0.368, val_acc: 91.060, lr: 0.200000\n","| Epoch [ 30/120] Iter[352/352]\t\tLoss: 0.8049 Acc@1: 79.878%, time: 1.226| Epoch: 30/120, val_loss: 0.735, val_acc: 71.220, lr: 0.200000\n","| Epoch: 30/120, val_loss: 0.304, val_acc: 91.000, lr: 0.200000\n","| Epoch [ 31/120] Iter[198/352]\t\tLoss: 0.7607 Acc@1: 80.560%, time: 1.946"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2SMIjoGjmj_B","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"33aad30c-2d2e-4e48-f009-3cdc035202c1","executionInfo":{"status":"ok","timestamp":1588695697910,"user_tz":-120,"elapsed":133227,"user":{"displayName":"Xavier López","photoUrl":"","userId":"17793741074420892205"}}},"source":["print('\\nTesting model')\n","checkpoint = torch.load(config.drive_dir + '/checkpoint/%s.pth.tar' % config.id)\n","test_net.load_state_dict(checkpoint['state_dict'])\n","test()\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["\n","Testing model\n","| test_loss: 0.158, test_acc: 92.320\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aprjXZNfmr-v","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"bf2db253-16d2-48be-853e-de2773b13868","executionInfo":{"status":"ok","timestamp":1588695937644,"user_tz":-120,"elapsed":681,"user":{"displayName":"Xavier López","photoUrl":"","userId":"17793741074420892205"}}},"source":["#This code is useful when the code has crashed without getting to the maximum of epochs (due to google collab session termination)\n","#essentially we reload weights and following continue the training process\n","\n","EPOCH = 39 #Input the last epoch computed\n","\n","print('Load Student')\n","checkpoint = torch.load(config.drive_dir + '/checkpoint/' + config.id + '_student_' + str(EPOCH) + '.pth.tar' )\n","net.load_state_dict(checkpoint['state_dict'])\n","\n","print('Load Teacher')\n","\n","checkpoint = torch.load(config.drive_dir + '/checkpoint/' + config.id + '_teacher_' + str(EPOCH) + '.pth.tar' )\n","tch_net.load_state_dict(checkpoint['state_dict'])"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Load Student\n","Load Teacher\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"W84wMN7xpNLP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"0a74c88c-27b7-4b8b-b02c-106af843027e"},"source":["init = False\n","best_acc = 0\n","for epoch in range(EPOCH, 1 + config.num_epochs):\n","    train(epoch)\n","    # Student validation\n","    std_results = valid(epoch, net)\n","    record.write(std_results + '\\n')\n","    print(std_results)\n","    # Teacher validation\n","    tch_results = valid(epoch, tch_net)\n","    record.write(tch_results + '\\n')\n","    record.flush()\n","    print(tch_results)\n","\n","    save_weights(epoch)\n","\n","print('\\nTesting model')\n","checkpoint = torch.load(config.drive_dir + '/checkpoint/%s.pth.tar' % config.id)\n","test_net.load_state_dict(checkpoint['state_dict'])\n","test()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2247: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch: 39/120, val_loss: 0.318, val_acc: 77.560, lr: 0.200000\n","| Epoch: 39/120, val_loss: 0.127, val_acc: 92.340, lr: 0.200000\n","| Epoch [ 40/120] Iter[352/352]\t\tLoss: 1.0688 Acc@1: 81.371%, time: 1.169"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f7296d752e8>>\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f7296d752e8>>\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n","    self._shutdown_workers()\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f7296d752e8>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n","    self._shutdown_workers()\n","    w.join()\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f7296d752e8>>\n","Traceback (most recent call last):\n","    w.join()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    w.join()\n","    self._shutdown_workers()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f7296d752e8>>\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","AssertionError: can only join a child process\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","    w.join()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f7296d752e8>>\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","    self._shutdown_workers()\n","AssertionError: can only join a child process\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    self._shutdown_workers()\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f7296d752e8>>\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f7296d752e8>>\n","AssertionError: can only join a child process\n","AssertionError: can only join a child process\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","    w.join()\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f7296d752e8>>\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","    self._shutdown_workers()\n","    self._shutdown_workers()\n","AssertionError: can only join a child process\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f7296d752e8>>\n","    self._shutdown_workers()\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n","    w.join()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n","    w.join()\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","    w.join()\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","AssertionError: can only join a child process\n","AssertionError: can only join a child process\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch: 40/120, val_loss: 0.649, val_acc: 77.540, lr: 0.200000\n","| Epoch: 40/120, val_loss: 0.125, val_acc: 92.400, lr: 0.200000\n","| Epoch [ 41/120] Iter[352/352]\t\tLoss: 0.7597 Acc@1: 81.709%, time: 1.165| Epoch: 41/120, val_loss: 0.725, val_acc: 77.360, lr: 0.200000\n","| Epoch: 41/120, val_loss: 0.126, val_acc: 92.120, lr: 0.200000\n","| Epoch [ 42/120] Iter[352/352]\t\tLoss: 0.9182 Acc@1: 81.813%, time: 1.166| Epoch: 42/120, val_loss: 0.251, val_acc: 76.840, lr: 0.200000\n","| Epoch: 42/120, val_loss: 0.139, val_acc: 92.500, lr: 0.200000\n","| Epoch [ 43/120] Iter[352/352]\t\tLoss: 0.6576 Acc@1: 82.091%, time: 1.172| Epoch: 43/120, val_loss: 0.732, val_acc: 68.600, lr: 0.200000\n","| Epoch: 43/120, val_loss: 0.126, val_acc: 92.700, lr: 0.200000\n","| Epoch [ 44/120] Iter[352/352]\t\tLoss: 0.7685 Acc@1: 82.191%, time: 1.169| Epoch: 44/120, val_loss: 0.298, val_acc: 80.020, lr: 0.200000\n","| Epoch: 44/120, val_loss: 0.144, val_acc: 92.580, lr: 0.200000\n","| Epoch [ 45/120] Iter[352/352]\t\tLoss: 0.4432 Acc@1: 82.427%, time: 1.173| Epoch: 45/120, val_loss: 0.509, val_acc: 76.680, lr: 0.200000\n","| Epoch: 45/120, val_loss: 0.141, val_acc: 92.800, lr: 0.200000\n","| Epoch [ 46/120] Iter[352/352]\t\tLoss: 0.6618 Acc@1: 82.444%, time: 1.171| Epoch: 46/120, val_loss: 0.622, val_acc: 80.060, lr: 0.200000\n","| Epoch: 46/120, val_loss: 0.123, val_acc: 92.840, lr: 0.200000\n","| Epoch [ 47/120] Iter[352/352]\t\tLoss: 0.7134 Acc@1: 82.429%, time: 1.169| Epoch: 47/120, val_loss: 0.192, val_acc: 77.000, lr: 0.200000\n","| Epoch: 47/120, val_loss: 0.119, val_acc: 93.080, lr: 0.200000\n","| Epoch [ 48/120] Iter[352/352]\t\tLoss: 0.7626 Acc@1: 82.738%, time: 1.170| Epoch: 48/120, val_loss: 1.176, val_acc: 80.100, lr: 0.200000\n","| Epoch: 48/120, val_loss: 0.128, val_acc: 93.120, lr: 0.200000\n","| Epoch [ 49/120] Iter[352/352]\t\tLoss: 0.5978 Acc@1: 82.660%, time: 1.174| Epoch: 49/120, val_loss: 0.273, val_acc: 80.680, lr: 0.200000\n","| Epoch: 49/120, val_loss: 0.118, val_acc: 93.300, lr: 0.200000\n","| Epoch [ 50/120] Iter[352/352]\t\tLoss: 0.6883 Acc@1: 82.962%, time: 1.174| Epoch: 50/120, val_loss: 0.689, val_acc: 75.060, lr: 0.200000\n","| Epoch: 50/120, val_loss: 0.114, val_acc: 93.160, lr: 0.200000\n","| Epoch [ 51/120] Iter[352/352]\t\tLoss: 0.5609 Acc@1: 82.809%, time: 1.167| Epoch: 51/120, val_loss: 0.489, val_acc: 79.260, lr: 0.200000\n","| Epoch: 51/120, val_loss: 0.113, val_acc: 93.420, lr: 0.200000\n","| Epoch [ 52/120] Iter[352/352]\t\tLoss: 0.7793 Acc@1: 83.287%, time: 1.166| Epoch: 52/120, val_loss: 0.231, val_acc: 78.380, lr: 0.200000\n","| Epoch: 52/120, val_loss: 0.106, val_acc: 93.420, lr: 0.200000\n","| Epoch [ 53/120] Iter[352/352]\t\tLoss: 0.5369 Acc@1: 83.316%, time: 1.167| Epoch: 53/120, val_loss: 0.404, val_acc: 79.980, lr: 0.200000\n","| Epoch: 53/120, val_loss: 0.108, val_acc: 93.240, lr: 0.200000\n","| Epoch [ 54/120] Iter[352/352]\t\tLoss: 0.9422 Acc@1: 83.300%, time: 1.169| Epoch: 54/120, val_loss: 0.386, val_acc: 68.880, lr: 0.200000\n","| Epoch: 54/120, val_loss: 0.096, val_acc: 93.120, lr: 0.200000\n","| Epoch [ 55/120] Iter[163/352]\t\tLoss: 0.7567 Acc@1: 83.579%, time: 1.844"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AFwEFJ3XUqDO","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v0KxssiuVjCi","colab_type":"code","colab":{}},"source":["# DISCARDING SAMPLES"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R0khwD7UUqQw","colab_type":"code","colab":{}},"source":["# RELOAD NETWORKS\n","EPOCH = 120 #Input the last epoch computed\n","\n","print('Load Student')\n","checkpoint = torch.load(config.drive_dir + '/checkpoint/' + config.id + '_student_' + str(EPOCH) + '.pth.tar' )\n","net.load_state_dict(checkpoint['state_dict'])\n","\n","print('Load Teacher')\n","\n","checkpoint = torch.load(config.drive_dir + '/checkpoint/' + config.id + '_teacher_' + str(EPOCH) + '.pth.tar' )\n","tch_net.load_state_dict(checkpoint['state_dict'])\n","\n","print('\\nLoad Test')\n","checkpoint = torch.load(config.drive_dir + '/checkpoint/%s.pth.tar' % config.id)\n","test_net.load_state_dict(checkpoint['state_dict'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SRzBS_nnVEnH","colab_type":"code","colab":{}},"source":["import pandas\n","def get_confidence_training(dataloader):\n","  correct = 0\n","  total = 0\n","\n","  target_list = []\n","  pred        = []\n","  confidence  = []\n","\n","  for batch_idx, (inputs, targets) in enumerate(dataloader):\n","      if use_cuda:\n","          inputs, targets = inputs.cuda(), targets.cuda()\n","      with torch.no_grad():\n","          outputs = test_net(inputs)\n","          loss = criterion(outputs, targets)  \n","          _, predicted = torch.max(outputs.data, 1)\n","          total += targets.size(0)\n","          correct += predicted.eq(targets.data).cpu().sum()\n","          \n","          conf = float(torch.max(outputs.softmax(dim = 1)))\n","          targ = int(targets.data)\n","          pd = int(predicted)\n","\n","          confidence.append(conf)\n","          target_list.append(targ)\n","          pred.append(pd)\n","\n","  # Grab dataframe\n","  df = pandas.DataFrame()\n","  df[\"target_list\"] = target_list\n","  df[\"pred\"] = pred\n","  df[\"confidence\"] = confidence\n","\n","  return(df)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rT1TFAbAUqat","colab_type":"code","colab":{}},"source":["#Those parameters are key for discarding\n","pre_batch = config.batch_size\n","config.batch_size = 1\n","config.shuffle = False\n","\n","# Get the dataloader with no shuffle\n","loader = dataloader.KeyDataLoader()\n","train_noshuffle_loader, _, _ = loader.run()\n","\n","\n","df = get_confidence_training(train_noshuffle_loader)\n","\n","#get list of directories\n","train_df = pandas.read_csv(config.data_dir+config.train_dir, header=None)\n","train_df.columns = [\"dir\"]\n","train_df\n","\n","#Merge directories with confidence\n","DF = df.merge(train_df,how = 'left', left_index = True, right_index = True)\n","\n","#filter only training samples that have a high enough confidence by the softmax\n","tau = 0.01\n","df_filtered = DF[DF.confidence > tau]\n","df_filtered\n","\n","df_filtered.to_csv(config.data_dir +'clean_train_key_list_iter2' + config.id + '.txt', header = None, index=False)\n","config.train_dir = 'clean_train_key_list_iter2' + config.id + '.txt'\n","\n","config.shuffle = True\n","config.batch_size = pre_batch\n","\n","# Get the filtered dataset\n","loader = dataloader.KeyDataLoader()\n","train_loader, _, _ = loader.run()\n","\n","#Initialize mentor\n","mentor_net = get_model()\n","if use_cuda:\n","    mentor_net.cuda()\n","\n","#Get the weights for the mentor\n","print('\\nLoad Mentor')\n","checkpoint = torch.load(config.drive_dir + '/checkpoint/%s.pth.tar' % config.id)\n","mentor_net.load_state_dict(checkpoint['state_dict'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1BXqDeVmUqyB","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VAMbXSV3Uq1E","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AWUwbhfNUqWk","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhqnhZ2YUqUE","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oyyFzY5r6yTV","colab_type":"text"},"source":[""]}]}