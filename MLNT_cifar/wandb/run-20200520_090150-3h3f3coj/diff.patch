diff --git a/MLNT_cifar/config.py b/MLNT_cifar/config.py
index cf371ff..bd84cf9 100644
--- a/MLNT_cifar/config.py
+++ b/MLNT_cifar/config.py
@@ -50,12 +50,19 @@ param_epoch = 20
 image_size = 32
 crop = 4
 
+# Hyper-params. (Vae)
+# -------------------------------------------- #
+vae_id = 'VAE'
+vae_model = 'AdaptiveVae'
+vae_weights = 'VAE_h500'
+vae_image_size = 128
+vae_crop = 0
 # -------------------------------------------- #
 
-#drive_dir = '.'
-#data_dir = './data/'
-drive_dir = '/content/drive/My Drive/Colab_Notebooks/git/PFM_Noisy_Labels/MLNT_cifar'
-data_dir =  '/content/drive/My Drive/Colab_Notebooks/git/PFM_Noisy_Labels/MLNT_cifar/data/'#'/data/'
+drive_dir = '.'
+data_dir = './data/'
+# drive_dir = '/content/drive/My Drive/Colab_Notebooks/git/PFM_Noisy_Labels/MLNT_cifar'
+# data_dir =  '/content/drive/My Drive/Colab_Notebooks/git/PFM_Noisy_Labels/MLNT_cifar/data/'#'/data/'
 
 #----------------------------------------------#
 #Dataloader files
diff --git a/MLNT_cifar/dataloader_vae.py b/MLNT_cifar/dataloader_vae.py
new file mode 100644
index 0000000..9af481a
--- /dev/null
+++ b/MLNT_cifar/dataloader_vae.py
@@ -0,0 +1,118 @@
+from torch.utils.data import Dataset, DataLoader
+import torchvision.transforms as transforms
+from PIL import Image
+import config
+
+
+class KeyDataset(Dataset):
+    def __init__(self, transform, mode, transform_2=None):
+        self.train_imgs = []
+        self.valid_imgs = []
+        self.test_imgs = []
+        self.train_labels = {}
+        self.valid_labels = {}
+        self.test_labels = {}
+        self.transform = transform
+        self.mode = mode
+        self.transform_2 = transform_2
+        with open(config.data_dir + config.train_dir, 'r') as f:
+            lines = f.read().splitlines()
+        for l in lines:
+            img_path = config.data_dir + l[7:]
+            self.train_imgs.append(img_path)
+
+        with open(config.data_dir + 'clean_test_key_list.txt', 'r') as f:
+            lines = f.read().splitlines()
+        for l in lines:
+            img_path = config.data_dir + l[7:]
+            self.test_imgs.append(img_path)
+
+        with open(config.data_dir + 'clean_val_key_list.txt', 'r') as f:
+            lines = f.read().splitlines()
+        for l in lines:
+            img_path = config.data_dir + l[7:]
+            self.valid_imgs.append(img_path)
+
+        with open(config.data_dir + config.train_labels_file, 'r') as f:
+            lines = f.read().splitlines()
+        for l in lines:
+            entry = l.split()
+            img_path = config.data_dir + entry[0][7:]
+            self.train_labels[img_path] = int(entry[1])
+
+        with open(config.data_dir + config.test_validation_labels_file, 'r') as f:
+            lines = f.read().splitlines()
+        for l in lines:
+            entry = l.split()
+            img_path = config.data_dir + entry[0][7:]
+            self.test_labels[img_path] = int(entry[1])
+
+    def __getitem__(self, index):
+        if self.mode == 'train':
+            img_path = self.train_imgs[index]
+            target = self.train_labels[img_path]
+        elif self.mode == 'valid':
+            img_path = self.valid_imgs[index]
+            target = self.test_labels[img_path]
+        elif self.mode == 'test':
+            img_path = self.test_imgs[index]
+            target = self.test_labels[img_path]
+        else:
+            raise Exception('%s not allowed'.format(self.mode))
+        image = Image.open(img_path).convert('RGB')
+
+        if self.transform_2 is not None:
+            return self.transform(image), self.transform_2(image), target
+        return self.transform(image), target
+
+    def __len__(self):
+        if self.mode == 'train':
+            return len(self.train_imgs)
+        elif self.mode == 'valid':
+            return len(self.valid_imgs)
+        elif self.mode == 'test':
+            return len(self.test_imgs)
+
+
+class KeyDataLoader(object):
+    def __init__(self):
+        self.transform_2 = None
+
+        self.transform_train = transforms.Compose([
+            transforms.RandomCrop(config.image_size, config.crop),
+            transforms.RandomHorizontalFlip(),
+            transforms.ToTensor(),
+            transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010)),
+
+        ])
+
+        self.transform_test = transforms.Compose([
+            transforms.ToTensor(),
+            transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010)),
+        ])
+
+    def add_transform(self, transform):
+        self.transform_2 = transform
+
+    def run(self):
+
+        train_loader = DataLoader(
+            dataset=KeyDataset(
+                transform=self.transform_train, mode='train', transform_2=self.transform_2),
+            batch_size=config.batch_size,
+            shuffle=config.shuffle,
+            num_workers=config.num_workers)
+        valid_loader = DataLoader(
+            dataset=KeyDataset(
+                transform=self.transform_test, mode='valid'),
+            batch_size=config.batch_size,
+            shuffle=False,
+            num_workers=config.num_workers)
+        test_loader = DataLoader(
+            dataset=KeyDataset(
+                transform=self.transform_test, mode='test'),
+            batch_size=config.batch_size,
+            shuffle=False,
+            num_workers=config.num_workers)
+
+        return train_loader, valid_loader, test_loader
diff --git a/MLNT_cifar/main_vae.ipynb b/MLNT_cifar/main_vae.ipynb
new file mode 100644
index 0000000..360d1ec
--- /dev/null
+++ b/MLNT_cifar/main_vae.ipynb
@@ -0,0 +1,1509 @@
+{
+ "cells": [
+  {
+   "cell_type": "code",
+   "execution_count": 1,
+   "metadata": {
+    "colab": {
+     "base_uri": "https://localhost:8080/",
+     "height": 1000
+    },
+    "colab_type": "code",
+    "id": "2AiEquVKc481",
+    "outputId": "c49e7790-62be-4c17-edf2-03c99f6ec6fe"
+   },
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Collecting wandb\n",
+      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/c9/ebbcefa6ef2ba14a7c62a4ee4415a5fecef8fac5e4d1b4e22af26fd9fe22/wandb-0.8.35-py2.py3-none-any.whl (1.4MB)\n",
+      "\u001b[K     |████████████████████████████████| 1.4MB 2.8MB/s \n",
+      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
+      "Collecting sentry-sdk>=0.4.0\n",
+      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/7e/19545324e83db4522b885808cd913c3b93ecc0c88b03e037b78c6a417fa8/sentry_sdk-0.14.3-py2.py3-none-any.whl (103kB)\n",
+      "\u001b[K     |████████████████████████████████| 112kB 47.3MB/s \n",
+      "\u001b[?25hRequirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.2)\n",
+      "Collecting docker-pycreds>=0.4.0\n",
+      "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
+      "Collecting shortuuid>=0.5.0\n",
+      "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
+      "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
+      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
+      "Collecting GitPython>=1.0.0\n",
+      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/33/917e6fde1cad13daa7053f39b7c8af3be287314f75f1b1ea8d3fe37a8571/GitPython-3.1.2-py3-none-any.whl (451kB)\n",
+      "\u001b[K     |████████████████████████████████| 460kB 47.5MB/s \n",
+      "\u001b[?25hRequirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.23.0)\n",
+      "Collecting configparser>=3.8.1\n",
+      "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n",
+      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
+      "Collecting subprocess32>=3.5.3\n",
+      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
+      "\u001b[K     |████████████████████████████████| 102kB 11.7MB/s \n",
+      "\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
+      "Collecting gql==0.2.0\n",
+      "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
+      "Collecting watchdog>=0.8.3\n",
+      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/c3/ed6d992006837e011baca89476a4bbffb0a91602432f73bd4473816c76e2/watchdog-0.10.2.tar.gz (95kB)\n",
+      "\u001b[K     |████████████████████████████████| 102kB 11.4MB/s \n",
+      "\u001b[?25hRequirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
+      "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.4.5.1)\n",
+      "Collecting gitdb<5,>=4.0.1\n",
+      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
+      "\u001b[K     |████████████████████████████████| 71kB 8.9MB/s \n",
+      "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.9)\n",
+      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
+      "Collecting graphql-core<2,>=0.5.0\n",
+      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n",
+      "\u001b[K     |████████████████████████████████| 71kB 8.8MB/s \n",
+      "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
+      "Collecting pathtools>=0.1.1\n",
+      "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
+      "Collecting smmap<4,>=3.0.1\n",
+      "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
+      "Building wheels for collected packages: subprocess32, gql, watchdog, graphql-core, pathtools\n",
+      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
+      "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=2dec55b8b47766d0fd875cd4aec6236c4be6c0d31b1f20da99aec685754426c7\n",
+      "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
+      "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
+      "  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=f3f0e9b0ea4e4d458f8318fadd3c88db3a3560fa5e8a9fad2de18f2455f7cd86\n",
+      "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
+      "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
+      "  Created wheel for watchdog: filename=watchdog-0.10.2-cp36-none-any.whl size=73605 sha256=9fa2fd21827b3c4392001b951b21debd896d3a48ed6a88798bb5b42c30da55f7\n",
+      "  Stored in directory: /root/.cache/pip/wheels/bc/ed/6c/028dea90d31b359cd2a7c8b0da4db80e41d24a59614154072e\n",
+      "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
+      "  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104650 sha256=475fc19d880ad4e02b86d226238b86700ca6b86e43c69c26b99475f412bcb0d2\n",
+      "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
+      "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
+      "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8784 sha256=0db4b1bf3b6a5f0bc46115c0344f07e74457310624a7e63e623ba795aa7ba6c5\n",
+      "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
+      "Successfully built subprocess32 gql watchdog graphql-core pathtools\n",
+      "Installing collected packages: sentry-sdk, docker-pycreds, shortuuid, smmap, gitdb, GitPython, configparser, subprocess32, graphql-core, gql, pathtools, watchdog, wandb\n",
+      "Successfully installed GitPython-3.1.2 configparser-5.0.0 docker-pycreds-0.4.0 gitdb-4.0.5 gql-0.2.0 graphql-core-1.1 pathtools-0.1.2 sentry-sdk-0.14.3 shortuuid-1.0.1 smmap-3.0.4 subprocess32-3.5.4 wandb-0.8.35 watchdog-0.10.2\n"
+     ]
+    }
+   ],
+   "source": [
+    "!pip install wandb"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 2,
+   "metadata": {
+    "colab": {
+     "base_uri": "https://localhost:8080/",
+     "height": 85
+    },
+    "colab_type": "code",
+    "id": "88ezc8iRdkqg",
+    "outputId": "d9f7219a-b54c-4d6b-95c7-d9f606ddbf01"
+   },
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://app.wandb.ai/authorize\n",
+      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: dff003aa03e7d25df35a840b6f0660ae9675efb4\n",
+      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
+      "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
+     ]
+    }
+   ],
+   "source": [
+    "!wandb login"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 3,
+   "metadata": {
+    "colab": {
+     "base_uri": "https://localhost:8080/",
+     "height": 307
+    },
+    "colab_type": "code",
+    "id": "8skRpihGdp1q",
+    "outputId": "9b9e0d6f-ddab-47b0-e134-f48ea37f41dd"
+   },
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Tue May 12 13:19:00 2020       \n",
+      "+-----------------------------------------------------------------------------+\n",
+      "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
+      "|-------------------------------+----------------------+----------------------+\n",
+      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
+      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
+      "|===============================+======================+======================|\n",
+      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
+      "| N/A   41C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
+      "+-------------------------------+----------------------+----------------------+\n",
+      "                                                                               \n",
+      "+-----------------------------------------------------------------------------+\n",
+      "| Processes:                                                       GPU Memory |\n",
+      "|  GPU       PID   Type   Process name                             Usage      |\n",
+      "|=============================================================================|\n",
+      "|  No running processes found                                                 |\n",
+      "+-----------------------------------------------------------------------------+\n"
+     ]
+    }
+   ],
+   "source": [
+    "#GPU INFO\n",
+    "gpu_info = !nvidia-smi\n",
+    "gpu_info = '\\n'.join(gpu_info)\n",
+    "if gpu_info.find('failed') >= 0:\n",
+    "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
+    "  print('and then re-execute this cell.')\n",
+    "else:\n",
+    "  print(gpu_info)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 4,
+   "metadata": {
+    "colab": {
+     "base_uri": "https://localhost:8080/",
+     "height": 122
+    },
+    "colab_type": "code",
+    "id": "csakjx4mdrs5",
+    "outputId": "31655575-3702-4c36-a019-e10a3cd5ccbd"
+   },
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
+      "\n",
+      "Enter your authorization code:\n",
+      "··········\n",
+      "Mounted at /content/drive/\n"
+     ]
+    }
+   ],
+   "source": [
+    "from google.colab import drive\n",
+    "drive.mount('/content/drive/')"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab": {},
+    "colab_type": "code",
+    "id": "zoV2tev1jHbQ"
+   },
+   "outputs": [],
+   "source": [
+    "import sys\n",
+    "sys.path.append('/content/drive/My Drive/Colab_Notebooks/git/PFM_Noisy_Labels/MLNT_cifar')"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 1,
+   "metadata": {
+    "colab": {
+     "base_uri": "https://localhost:8080/",
+     "height": 119
+    },
+    "colab_type": "code",
+    "id": "Xq88YZuGhRTA",
+    "outputId": "bf0353d3-cbd2-4172-b20b-9d5af0558def"
+   },
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "\n",
+      "noise file noisy_label_kv50.txt generated with noise: 0.5\n",
+      "\n"
+     ]
+    },
+    {
+     "data": {
+      "text/html": [
+       "\n",
+       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
+       "                Project page: <a href=\"https://app.wandb.ai/jordiventura/Cifar_Noise\" target=\"_blank\">https://app.wandb.ai/jordiventura/Cifar_Noise</a><br/>\n",
+       "                Run page: <a href=\"https://app.wandb.ai/jordiventura/Cifar_Noise/runs/anzv78aw\" target=\"_blank\">https://app.wandb.ai/jordiventura/Cifar_Noise/runs/anzv78aw</a><br/>\n",
+       "            "
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "wandb: Wandb version 0.8.36 is available!  To upgrade, please run:\n",
+      "wandb:  $ pip install wandb --upgrade\n"
+     ]
+    }
+   ],
+   "source": [
+    "# Pytorch libraries\n",
+    "import torch\n",
+    "import torch.nn.functional as F\n",
+    "import torch.backends.cudnn as cudnn\n",
+    "\n",
+    "# Internal files\n",
+    "import config\n",
+    "import dataloader_vae as dataloader\n",
+    "import models\n",
+    "# from baseline import get_model, save_checkpoint\n",
+    "\n",
+    "import math\n",
+    "import os\n",
+    "import sys\n",
+    "import time\n",
+    "from collections import OrderedDict\n",
+    "import random\n",
+    "\n",
+    "import wandb\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab": {},
+    "colab_type": "code",
+    "id": "tsMIDod2mpPz"
+   },
+   "outputs": [],
+   "source": [
+    "def get_model():\n",
+    "\n",
+    "    # Get model from config\n",
+    "    if config.model == \"resnet18\":\n",
+    "        model = models.resnet18(pretrained=config.pretrained)\n",
+    "    elif config.model == \"resnet34\":\n",
+    "        model = models.resnet34(pretrained=config.pretrained)\n",
+    "    elif config.model == 'resnet50':\n",
+    "        model = models.resnet50(pretrained=config.pretrained)\n",
+    "    elif config.model == \"resnet101\":\n",
+    "        model = models.resnet101(pretrained=config.pretrained)\n",
+    "    elif config.model == \"resnet152\":\n",
+    "        model = models.resnet152(pretrained=config.pretrained)\n",
+    "    elif config.model == \"resnext50_32x4d\":\n",
+    "        model = models.resnet34(pretrained=config.pretrained)\n",
+    "    elif config.model == 'resnext101_32x8d':\n",
+    "        model = models.resnet50(pretrained=config.pretrained)\n",
+    "    elif config.model == \"wide_resnet50_2\":\n",
+    "        model = models.resnet101(pretrained=config.pretrained)\n",
+    "    elif config.model == \"wide_resnet101_2\":\n",
+    "        model = models.resnet152(pretrained=config.pretrained)\n",
+    "    else:\n",
+    "        raise ValueError('%s not supported'.format(config.model))\n",
+    "\n",
+    "    # Initialize fc layer\n",
+    "    (in_features, out_features) = model.fc.in_features, model.fc.out_features\n",
+    "    model.fc = torch.nn.Linear(in_features, out_features)\n",
+    "    return model\n",
+    "\n",
+    "\n",
+    "\n",
+    "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
+    "    torch.save(state, filename)\n",
+    "    if config.use_wandb == True:\n",
+    "        wandb.save(filename)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "#### Variational AE."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 1,
+   "metadata": {},
+   "outputs": [
+    {
+     "ename": "ImportError",
+     "evalue": "cannot import name 'AdaptiveVAE'",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
+      "\u001b[1;32m<ipython-input-1-39caa01967c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdaptiveVAE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdataloader_vae\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;31mImportError\u001b[0m: cannot import name 'AdaptiveVAE'"
+     ]
+    }
+   ],
+   "source": [
+    "import torch\n",
+    "from models import AdaptiveVAE\n",
+    "import dataloader_vae as dataloader\n",
+    "\n",
+    "\n",
+    "def Bhattacharyya_dist(par1: tuple, par2: tuple):\n",
+    "\n",
+    "    with torch.no_grad():\n",
+    "        mu1, std1 = par1\n",
+    "        mu2, std2 = par2\n",
+    "\n",
+    "        cov = 1/2 * (std1 + std2)\n",
+    "\n",
+    "        mah_term = 1/8 * torch.sum((mu1 - mu2) * cov * (mu1 - mu2))\n",
+    "\n",
+    "        det_cov1 = torch.prod(std1, 0)\n",
+    "        det_cov2 = torch.prod(std2, 0)\n",
+    "\n",
+    "        aux_term = 1/2 * torch.log(torch.prod(cov, 0) / det_cov1.mul(det_cov2).pow(1/2))\n",
+    "\n",
+    "    return mah_term + aux_term\n",
+    "\n",
+    "\n",
+    "# Hyper Parameter settings\n",
+    "img = 128\n",
+    "crop = 0\n",
+    "hid_size = 500\n",
+    "ker = 3\n",
+    "strides = (2, 2, 2, 2, 2)\n",
+    "leaky = (1, 1)\n",
+    "lr = 1e-4\n",
+    "log_interval = 10\n",
+    "\n",
+    "# Networks setup\n",
+    "print('\\nModel setup')\n",
+    "print('| Building network: AdaptiveVAE(hid_size=%s)' % hid_size)\n",
+    "print('| Input image size: %s' % img)\n",
+    "vae = AdaptiveVAE(img=img, hid_size=hid_size, ker=ker, strides=strides, leaky=leaky)\n",
+    "\n",
+    "checkpoint_vae = torch.load(config.drive_dir + '/checkpoint/VAE_h%s.pth.tar' % hid_size,\n",
+    "                            map_location='cpu')\n",
+    "vae_net.load_state_dict(checkpoint_vae['state_dict'])\n",
+    "\n",
+    "# Get the original_dataset\n",
+    "loader = dataloader.KeyDataLoader()\n",
+    "loader.add_transform(\n",
+    "    transforms.Compose([\n",
+    "        transforms.Resize(config.vae_image_size + config.vae_crop),\n",
+    "        transforms.CenterCrop(config.vae_image_size),\n",
+    "        transforms.ToTensor(),\n",
+    "    ]))\n",
+    "train_loader, valid_loader, test_loader = loader.run()\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab": {},
+    "colab_type": "code",
+    "id": "UG1lpyQugVk3"
+   },
+   "outputs": [],
+   "source": [
+    "def scheduler(epoch: int):\n",
+    "    global lr\n",
+    "    lr = config.lr\n",
+    "    if epoch > config.start_epoch:\n",
+    "        lr = lr / 10.0\n",
+    "    for param_group in optimizer.param_groups:\n",
+    "        param_group['lr'] = lr\n",
+    "\n",
+    "# Training\n",
+    "def train(epoch):\n",
+    "    global init\n",
+    "    net.train()\n",
+    "    tch_net.train()\n",
+    "    train_loss = 0\n",
+    "    correct = 0\n",
+    "    total = 0\n",
+    "    scheduler(epoch)\n",
+    "\n",
+    "\n",
+    "    # ramp up meta-learning rate and EMA decay\n",
+    "    if epoch <= config.param_epoch:\n",
+    "        u = epoch/config.param_epoch\n",
+    "        meta_lr = config.meta_lr*math.exp(-5*(1-u)**2)\n",
+    "        lamb = 0.5*math.exp(-5*(1-u)**2)\n",
+    "    else:\n",
+    "        meta_lr = config.meta_lr\n",
+    "        config.eps = 0.999\n",
+    "\n",
+    "    for step, (inputs, inputs_vae, targets) in enumerate(train_loader):\n",
+    "        init_time = time.time()\n",
+    "        if use_cuda:\n",
+    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
+    "        optimizer.zero_grad()\n",
+    "        outputs = net(inputs)\n",
+    "\n",
+    "        class_loss = criterion(outputs, targets)\n",
+    "        class_loss.backward(retain_graph=True)\n",
+    "\n",
+    "\n",
+    "        if step > config.start_iter or epoch > 1:\n",
+    "        #if step > 0 or epoch > 0:\n",
+    "\n",
+    "            # if step > config.mid_iter or epoch > 1:\n",
+    "            #     # config.eps = 0.999\n",
+    "            #     alpha = config.alpha\n",
+    "            # else:\n",
+    "            #     u = (step - config.start_iter)/(config.mid_iter - config.start_iter)\n",
+    "            #     alpha = config.alpha*math.exp(-5*(1-u)**2)\n",
+    "            alpha = config.alpha\n",
+    "\n",
+    "            if init:\n",
+    "                init = False\n",
+    "                for param, param_tch in zip(net.parameters(), tch_net.parameters()):\n",
+    "                    param_tch.data.copy_(param.data)\n",
+    "            else:\n",
+    "                for param, param_tch in zip(net.parameters(), tch_net.parameters()):\n",
+    "                    param_tch.data.mul_(config.eps).add_((1-config.eps), param.data)\n",
+    "\n",
+    "            # _, feats = pretrain_net(inputs, get_feat=True)\n",
+    "            with torch.no_grad():\n",
+    "                mu, std = vae_net.encode(inputs_vae)\n",
+    "            \n",
+    "            tch_outputs = tch_net(inputs, get_feat=False)\n",
+    "            p_tch = F.softmax(tch_outputs, dim=1)\n",
+    "            p_tch.detach_()\n",
+    "\n",
+    "            if use_mentor == True:\n",
+    "                mnt_outputs = mentor_net(inputs, get_feat=False)\n",
+    "                p_mnt = F.softmax(tch_outputs, dim=1)\n",
+    "                p_mnt.detach_()\n",
+    "\n",
+    "            for i in range(config.num_fast):\n",
+    "                targets_fast = targets.clone()\n",
+    "                randidx = torch.randperm(targets.size(0))\n",
+    "                for n in range(int(targets.size(0)*config.perturb_ratio)):\n",
+    "                    num_neighbor = 10\n",
+    "                    idx = randidx[n]\n",
+    "                    feat = feats[idx]\n",
+    "                    feat.view(1, feat.size(0))\n",
+    "                    feat.data = feat.data.expand(targets.size(0), feat.size(0))\n",
+    "                    dist = torch.sum((feat-feats)**2, dim=1)\n",
+    "                    _, neighbor = torch.topk(dist.data, num_neighbor+1, largest=False)\n",
+    "                    targets_fast[idx] = targets[neighbor[random.randint(1, num_neighbor)]]\n",
+    "\n",
+    "                fast_loss = criterion(outputs, targets_fast)\n",
+    "\n",
+    "                grads = torch.autograd.grad(fast_loss, net.parameters(),\n",
+    "                                            create_graph=False,\n",
+    "                                            retain_graph=True,\n",
+    "                                            only_inputs=True)\n",
+    "\n",
+    "                fast_weights = OrderedDict(\n",
+    "                    (name, param - meta_lr*grad)\n",
+    "                    for ((name, param), grad) in zip(net.named_parameters(), grads))\n",
+    "\n",
+    "                fast_out = net.forward(inputs,fast_weights)\n",
+    "\n",
+    "                logp_fast = F.log_softmax(fast_out,dim=1)\n",
+    "\n",
+    "                #afegir canvis per iterative aquí\n",
+    "                if use_mentor == False:\n",
+    "                    consistent_loss = consistent_criterion(logp_fast, p_tch)\n",
+    "                else:\n",
+    "                    consistent_loss = consistent_criterion(logp_fast, p_tch*lamb + p_mnt*(1-lamb))\n",
+    "\n",
+    "                consistent_loss = consistent_loss*alpha/config.num_fast\n",
+    "                consistent_loss.backward()\n",
+    "\n",
+    "        optimizer.step()\n",
+    "\n",
+    "        # train_loss += class_loss.data.item()\n",
+    "        _, predicted = torch.max(outputs.data, 1)\n",
+    "        total += targets.size(0)\n",
+    "        correct += predicted.eq(targets.data).cpu().sum()\n",
+    "\n",
+    "        # Grab training results\n",
+    "        sys.stdout.write('\\r')\n",
+    "        sys.stdout.write('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%, time: %.3f'\n",
+    "              %(epoch, config.num_epochs, step+1, (len(train_loader.dataset)//config.batch_size)+1, class_loss.data.item(), 100.*correct/total,time.time() - init_time))\n",
+    "        sys.stdout.flush()\n",
+    "\n",
+    "\n",
+    "\n",
+    "def valid(epoch, network):\n",
+    "    global best_acc\n",
+    "    network.eval()\n",
+    "    # val_loss = 0\n",
+    "    correct = 0\n",
+    "    total = 0\n",
+    "    for step, (inputs, targets) in enumerate(valid_loader):\n",
+    "        if use_cuda:\n",
+    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
+    "        with torch.no_grad():\n",
+    "            outputs = network(inputs)\n",
+    "            loss = criterion(outputs, targets)\n",
+    "\n",
+    "        # valid_loss += loss.data.item()\n",
+    "        _, predicted = torch.max(outputs.data, 1)\n",
+    "        total += targets.size(0)\n",
+    "        correct += predicted.eq(targets.data).cpu().sum()\n",
+    "\n",
+    "        # Grab validation results\n",
+    "        valid_acc = 100. * correct / total\n",
+    "      # valid_results = (\"| Epoch: {}/{}, val_loss: {:.3f}, val_acc: {:.3f}, \"\n",
+    "      #                 \"lr: {:.6f}\".format(epoch,\n",
+    "      #                                     config.num_epochs,\n",
+    "      #                                     loss.data.item(),\n",
+    "      #                                     valid_acc,\n",
+    "      #                                     lr))\n",
+    "        # Grab validation results\n",
+    "        valid_results = (\"| Epoch: {}/{}, val_loss: {:.3f}, val_acc: {:.3f}, \"\"lr: {:.6f}\".format(epoch,config.num_epochs,loss.data.item(),valid_acc,lr))\n",
+    "        record.write(valid_results + '\\n')\n",
+    "        record.flush()\n",
+    "\n",
+    "\n",
+    "\n",
+    "    # Save checkpoint when best model\n",
+    "    if valid_acc > best_acc:\n",
+    "        best_acc = valid_acc\n",
+    "        print('| Saving Best Model ...', end=\"\\r\")\n",
+    "        save_point = config.drive_dir + '/checkpoint/' + config.id + '.pth.tar'\n",
+    "        save_checkpoint({\n",
+    "            'state_dict': network.state_dict(),\n",
+    "            'best_acc': best_acc,\n",
+    "        }, save_point)\n",
+    "\n",
+    "    wandb.log({'epoch': epoch, 'accy_val' : best_acc })\n",
+    "\n",
+    "    return valid_results\n",
+    "\n",
+    "\n",
+    "def test():\n",
+    "    test_net.eval()\n",
+    "    # test_loss = 0\n",
+    "    correct = 0\n",
+    "    total = 0\n",
+    "    for batch_idx, (inputs, targets) in enumerate(valid_loader):\n",
+    "        if use_cuda:\n",
+    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
+    "        with torch.no_grad():\n",
+    "            outputs = test_net(inputs)\n",
+    "            loss = criterion(outputs, targets)\n",
+    "\n",
+    "        # test_loss += loss.data.item()\n",
+    "        _, predicted = torch.max(outputs.data, 1)\n",
+    "        total += targets.size(0)\n",
+    "        correct += predicted.eq(targets.data).cpu().sum()\n",
+    "\n",
+    "    # Grab validation results\n",
+    "    test_acc = 100. * correct/total\n",
+    "    test_results = \"| test_loss: {:.3f}, test_acc: {:.3f}\".format(\n",
+    "        loss.data.item(), test_acc)\n",
+    "    record.write(test_results)\n",
+    "    record.flush()\n",
+    "\n",
+    "    print(test_results)\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab": {},
+    "colab_type": "code",
+    "id": "NEBEekTkLEP4"
+   },
+   "outputs": [],
+   "source": [
+    "def save_weights(epoch):\n",
+    "        print('| Saving Weights student ...', end=\"\\r\")\n",
+    "        save_point = config.drive_dir + '/checkpoint/' + config.id + '_student_' + str(epoch) + '.pth.tar'\n",
+    "        save_checkpoint({'state_dict': net.state_dict(), }, save_point)\n",
+    "\n",
+    "        print('| Saving Weights teacher ...', end=\"\\r\")\n",
+    "        save_point = config.drive_dir + '/checkpoint/' + config.id + '_teacher_' + str(epoch) + '.pth.tar'\n",
+    "        save_checkpoint({'state_dict': tch_net.state_dict(), }, save_point)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 10,
+   "metadata": {
+    "colab": {
+     "base_uri": "https://localhost:8080/",
+     "height": 273
+    },
+    "colab_type": "code",
+    "id": "9mjWKzpX9L_W",
+    "outputId": "0fbe126f-7bb0-4b88-e586-b788de9d9d38"
+   },
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "\n",
+      "Model setup\n",
+      "| Building network: resnet34\n"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "/content/drive/My Drive/Colab_Notebooks/git/PFM_Noisy_Labels/MLNT_cifar/models/resnet.py:122: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
+      "  nn.init.kaiming_normal(m.weight, mode='fan_out')\n",
+      "/content/drive/My Drive/Colab_Notebooks/git/PFM_Noisy_Labels/MLNT_cifar/models/resnet.py:124: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
+      "  nn.init.constant(m.weight, 1)\n",
+      "/content/drive/My Drive/Colab_Notebooks/git/PFM_Noisy_Labels/MLNT_cifar/models/resnet.py:125: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
+      "  nn.init.constant(m.bias, 0)\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "| load pretrained net. from checkpoint...\n",
+      "\n",
+      "Training model\n",
+      "| Training Epochs = 120\n",
+      "| Initial Learning Rate = 0.2\n",
+      "| Optimizer = SGD\n"
+     ]
+    }
+   ],
+   "source": [
+    "# Checkpoint dir.\n",
+    "record = open(config.drive_dir + '/checkpoint/' + config.checkpoint + '_test.txt', 'w')\n",
+    "record.write('noise_rate=%s\\n' % config.noise_rate)\n",
+    "record.flush()\n",
+    "\n",
+    "# # Get the original_dataset\n",
+    "# loader = dataloader.KeyDataLoader()\n",
+    "# train_loader, valid_loader, test_loader = loader.run()\n",
+    "\n",
+    "# Hyper Parameter settings\n",
+    "random.seed(config.seed)\n",
+    "# torch.cuda.set_device(config.gpuid)\n",
+    "torch.manual_seed(config.seed)\n",
+    "torch.cuda.manual_seed_all(config.seed)\n",
+    "use_cuda = torch.cuda.is_available()\n",
+    "\n",
+    "# Networks setup\n",
+    "print('\\nModel setup')\n",
+    "print('| Building network: {}'.format(config.model))\n",
+    "net = get_model()\n",
+    "tch_net = get_model()\n",
+    "pretrain_net = get_model()\n",
+    "test_net = get_model()\n",
+    "\n",
+    "print('| load pretrained net. from checkpoint...')\n",
+    "checkpoint = torch.load(config.drive_dir + '/checkpoint/' + config.checkpoint + '.pth.tar')\n",
+    "pretrain_net.load_state_dict(checkpoint['state_dict'])\n",
+    "\n",
+    "if use_cuda:\n",
+    "    net.cuda()\n",
+    "    tch_net.cuda()\n",
+    "    pretrain_net.cuda()\n",
+    "    test_net.cuda()\n",
+    "    cudnn.benchmark = True\n",
+    "pretrain_net.eval()\n",
+    "\n",
+    "for param in tch_net.parameters():\n",
+    "    param.requires_grad = False\n",
+    "for param in pretrain_net.parameters():\n",
+    "    param.requires_grad = False\n",
+    "\n",
+    "# Instantiate a loss function.\n",
+    "criterion = torch.nn.CrossEntropyLoss()\n",
+    "consistent_criterion = torch.nn.KLDivLoss()\n",
+    "\n",
+    "# Instantiate an optimizer to train the model\n",
+    "optimizer = torch.optim.SGD(\n",
+    "    net.parameters(), lr=config.lr, momentum=config.momentum, weight_decay=config.weight_decay)\n",
+    "\n",
+    "print('\\nTraining model')\n",
+    "print('| Training Epochs = ' + str(config.num_epochs))\n",
+    "print('| Initial Learning Rate = ' + str(config.lr))\n",
+    "print('| Optimizer = ' + str(config.optimizer_type))\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab": {
+     "base_uri": "https://localhost:8080/",
+     "height": 1000
+    },
+    "colab_type": "code",
+    "id": "bZRHtbXGmqnM",
+    "outputId": "e7d982e2-d905-47ec-a6ab-d058f5778d41"
+   },
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "| Epoch: 1/120, val_loss: 2.826, val_acc: 17.920, lr: 0.200000\n",
+      "| Epoch: 1/120, val_loss: 0.003, val_acc: 10.000, lr: 0.200000\n"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2247: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
+      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "\r",
+      "| Epoch [  2/120] Iter[  1/352]\t\tLoss: 2.0485 Acc@1: 14.062%, time: 2.379"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
+      "\tadd_(Number alpha, Tensor other)\n",
+      "Consider using one of the following signatures instead:\n",
+      "\tadd_(Tensor other, *, Number alpha)\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "| Epoch: 2/120, val_loss: 2.905, val_acc: 21.800, lr: 0.200000\n",
+      "| Epoch: 2/120, val_loss: 2.723, val_acc: 24.680, lr: 0.200000\n",
+      "| Epoch: 3/120, val_loss: 2.288, val_acc: 29.020, lr: 0.200000\n",
+      "| Epoch: 3/120, val_loss: 2.187, val_acc: 30.800, lr: 0.200000\n",
+      "| Epoch: 4/120, val_loss: 2.336, val_acc: 31.380, lr: 0.200000\n",
+      "| Epoch: 4/120, val_loss: 1.954, val_acc: 36.260, lr: 0.200000\n",
+      "| Epoch [  5/120] Iter[352/352]\t\tLoss: 1.6336 Acc@1: 38.927%, time: 1.175| Epoch: 5/120, val_loss: 2.090, val_acc: 35.380, lr: 0.200000\n",
+      "| Epoch: 5/120, val_loss: 1.763, val_acc: 41.040, lr: 0.200000\n",
+      "| Epoch [  6/120] Iter[352/352]\t\tLoss: 1.6814 Acc@1: 42.862%, time: 1.175| Epoch: 6/120, val_loss: 1.905, val_acc: 38.860, lr: 0.200000\n",
+      "| Epoch: 6/120, val_loss: 1.633, val_acc: 46.180, lr: 0.200000\n",
+      "| Epoch [  7/120] Iter[352/352]\t\tLoss: 1.2168 Acc@1: 47.084%, time: 1.169| Epoch: 7/120, val_loss: 1.482, val_acc: 43.700, lr: 0.200000\n",
+      "| Epoch: 7/120, val_loss: 1.635, val_acc: 51.960, lr: 0.200000\n",
+      "| Epoch [  8/120] Iter[352/352]\t\tLoss: 1.3172 Acc@1: 51.022%, time: 1.175| Epoch: 8/120, val_loss: 1.487, val_acc: 50.180, lr: 0.200000\n",
+      "| Epoch: 8/120, val_loss: 1.521, val_acc: 59.100, lr: 0.200000\n",
+      "| Epoch [  9/120] Iter[352/352]\t\tLoss: 1.0498 Acc@1: 55.618%, time: 1.177| Epoch: 9/120, val_loss: 1.566, val_acc: 51.580, lr: 0.200000\n",
+      "| Epoch: 9/120, val_loss: 1.338, val_acc: 63.600, lr: 0.200000\n",
+      "| Epoch [ 10/120] Iter[352/352]\t\tLoss: 0.9983 Acc@1: 59.131%, time: 1.176| Epoch: 10/120, val_loss: 2.086, val_acc: 48.760, lr: 0.200000\n",
+      "| Epoch: 10/120, val_loss: 1.269, val_acc: 68.400, lr: 0.200000\n",
+      "| Epoch [ 11/120] Iter[352/352]\t\tLoss: 0.9745 Acc@1: 62.031%, time: 1.178| Epoch: 11/120, val_loss: 1.035, val_acc: 58.840, lr: 0.200000\n",
+      "| Epoch: 11/120, val_loss: 1.055, val_acc: 72.440, lr: 0.200000\n",
+      "| Epoch [ 12/120] Iter[352/352]\t\tLoss: 0.7591 Acc@1: 64.349%, time: 1.187| Epoch: 12/120, val_loss: 1.298, val_acc: 65.820, lr: 0.200000\n",
+      "| Epoch: 12/120, val_loss: 0.909, val_acc: 76.000, lr: 0.200000\n",
+      "| Epoch [ 13/120] Iter[352/352]\t\tLoss: 0.9313 Acc@1: 66.418%, time: 1.178| Epoch: 13/120, val_loss: 0.927, val_acc: 64.960, lr: 0.200000\n",
+      "| Epoch: 13/120, val_loss: 0.939, val_acc: 77.040, lr: 0.200000\n",
+      "| Epoch [ 14/120] Iter[352/352]\t\tLoss: 0.8138 Acc@1: 68.169%, time: 1.181| Epoch: 14/120, val_loss: 1.532, val_acc: 65.500, lr: 0.200000\n",
+      "| Epoch: 14/120, val_loss: 0.854, val_acc: 79.440, lr: 0.200000\n",
+      "| Epoch [ 15/120] Iter[352/352]\t\tLoss: 0.7713 Acc@1: 68.947%, time: 1.177| Epoch: 15/120, val_loss: 1.335, val_acc: 59.960, lr: 0.200000\n",
+      "| Epoch: 15/120, val_loss: 0.703, val_acc: 81.440, lr: 0.200000\n",
+      "| Epoch [ 16/120] Iter[352/352]\t\tLoss: 0.7919 Acc@1: 70.289%, time: 1.173| Epoch: 16/120, val_loss: 1.542, val_acc: 62.340, lr: 0.200000\n",
+      "| Epoch: 16/120, val_loss: 0.735, val_acc: 81.600, lr: 0.200000\n",
+      "| Epoch [ 17/120] Iter[352/352]\t\tLoss: 0.7235 Acc@1: 70.909%, time: 1.172| Epoch: 17/120, val_loss: 1.550, val_acc: 68.000, lr: 0.200000\n",
+      "| Epoch: 17/120, val_loss: 0.730, val_acc: 83.240, lr: 0.200000\n",
+      "| Epoch [ 18/120] Iter[352/352]\t\tLoss: 0.6553 Acc@1: 72.078%, time: 1.178| Epoch: 18/120, val_loss: 1.610, val_acc: 64.320, lr: 0.200000\n",
+      "| Epoch: 18/120, val_loss: 0.737, val_acc: 84.020, lr: 0.200000\n",
+      "| Epoch [ 19/120] Iter[352/352]\t\tLoss: 0.7280 Acc@1: 72.336%, time: 1.180| Epoch: 19/120, val_loss: 0.387, val_acc: 64.120, lr: 0.200000\n",
+      "| Epoch: 19/120, val_loss: 0.708, val_acc: 84.280, lr: 0.200000\n",
+      "| Epoch [ 20/120] Iter[352/352]\t\tLoss: 0.6833 Acc@1: 72.936%, time: 1.173| Epoch: 20/120, val_loss: 0.916, val_acc: 71.360, lr: 0.200000\n",
+      "| Epoch: 20/120, val_loss: 0.560, val_acc: 85.000, lr: 0.200000\n",
+      "| Epoch [ 21/120] Iter[352/352]\t\tLoss: 0.4942 Acc@1: 73.264%, time: 1.171| Epoch: 21/120, val_loss: 1.115, val_acc: 63.580, lr: 0.200000\n",
+      "| Epoch: 21/120, val_loss: 0.564, val_acc: 85.200, lr: 0.200000\n",
+      "| Epoch [ 22/120] Iter[352/352]\t\tLoss: 0.6663 Acc@1: 73.924%, time: 1.181| Epoch: 22/120, val_loss: 0.985, val_acc: 63.140, lr: 0.200000\n",
+      "| Epoch: 22/120, val_loss: 0.571, val_acc: 85.820, lr: 0.200000\n",
+      "| Epoch [ 23/120] Iter[352/352]\t\tLoss: 0.6426 Acc@1: 74.242%, time: 1.176| Epoch: 23/120, val_loss: 0.827, val_acc: 73.520, lr: 0.200000\n",
+      "| Epoch: 23/120, val_loss: 0.596, val_acc: 86.200, lr: 0.200000\n",
+      "| Epoch [ 24/120] Iter[352/352]\t\tLoss: 0.5371 Acc@1: 74.629%, time: 1.174| Epoch: 24/120, val_loss: 0.647, val_acc: 75.460, lr: 0.200000\n",
+      "| Epoch: 24/120, val_loss: 0.558, val_acc: 87.000, lr: 0.200000\n",
+      "| Epoch [ 25/120] Iter[352/352]\t\tLoss: 0.5299 Acc@1: 74.618%, time: 1.173| Epoch: 25/120, val_loss: 0.393, val_acc: 74.620, lr: 0.200000\n",
+      "| Epoch: 25/120, val_loss: 0.575, val_acc: 87.040, lr: 0.200000\n",
+      "| Epoch [ 26/120] Iter[352/352]\t\tLoss: 0.7187 Acc@1: 75.071%, time: 1.177| Epoch: 26/120, val_loss: 1.067, val_acc: 67.060, lr: 0.200000\n",
+      "| Epoch: 26/120, val_loss: 0.558, val_acc: 87.560, lr: 0.200000\n",
+      "| Epoch [ 27/120] Iter[352/352]\t\tLoss: 0.4703 Acc@1: 75.447%, time: 1.179| Epoch: 27/120, val_loss: 0.767, val_acc: 75.860, lr: 0.200000\n",
+      "| Epoch: 27/120, val_loss: 0.601, val_acc: 87.640, lr: 0.200000\n",
+      "| Epoch [ 28/120] Iter[352/352]\t\tLoss: 0.4107 Acc@1: 75.658%, time: 1.176| Epoch: 28/120, val_loss: 2.204, val_acc: 69.640, lr: 0.200000\n",
+      "| Epoch: 28/120, val_loss: 0.567, val_acc: 87.920, lr: 0.200000\n",
+      "| Epoch [ 29/120] Iter[352/352]\t\tLoss: 0.4566 Acc@1: 75.816%, time: 1.168| Epoch: 29/120, val_loss: 1.381, val_acc: 73.280, lr: 0.200000\n",
+      "| Epoch: 29/120, val_loss: 0.582, val_acc: 87.800, lr: 0.200000\n",
+      "| Epoch [ 30/120] Iter[352/352]\t\tLoss: 0.4691 Acc@1: 76.198%, time: 1.169| Epoch: 30/120, val_loss: 2.321, val_acc: 59.900, lr: 0.200000\n",
+      "| Epoch: 30/120, val_loss: 0.562, val_acc: 87.860, lr: 0.200000\n",
+      "| Epoch [ 31/120] Iter[352/352]\t\tLoss: 0.4941 Acc@1: 76.244%, time: 1.173| Epoch: 31/120, val_loss: 0.963, val_acc: 65.880, lr: 0.200000\n",
+      "| Epoch: 31/120, val_loss: 0.552, val_acc: 88.160, lr: 0.200000\n",
+      "| Epoch [ 32/120] Iter[352/352]\t\tLoss: 0.3781 Acc@1: 76.360%, time: 1.182| Epoch: 32/120, val_loss: 1.023, val_acc: 64.420, lr: 0.200000\n",
+      "| Epoch: 32/120, val_loss: 0.542, val_acc: 88.160, lr: 0.200000\n",
+      "| Epoch [ 33/120] Iter[352/352]\t\tLoss: 0.6685 Acc@1: 76.767%, time: 1.179| Epoch: 33/120, val_loss: 0.511, val_acc: 72.240, lr: 0.200000\n",
+      "| Epoch: 33/120, val_loss: 0.545, val_acc: 88.440, lr: 0.200000\n",
+      "| Epoch [ 34/120] Iter[352/352]\t\tLoss: 0.3607 Acc@1: 76.736%, time: 1.183| Epoch: 34/120, val_loss: 0.781, val_acc: 67.740, lr: 0.200000\n",
+      "| Epoch: 34/120, val_loss: 0.539, val_acc: 88.580, lr: 0.200000\n",
+      "| Epoch [ 35/120] Iter[352/352]\t\tLoss: 0.5861 Acc@1: 77.247%, time: 1.176| Epoch: 35/120, val_loss: 1.067, val_acc: 74.800, lr: 0.200000\n",
+      "| Epoch: 35/120, val_loss: 0.541, val_acc: 88.900, lr: 0.200000\n",
+      "| Epoch [ 36/120] Iter[352/352]\t\tLoss: 0.5002 Acc@1: 77.084%, time: 1.176| Epoch: 36/120, val_loss: 0.662, val_acc: 74.940, lr: 0.200000\n",
+      "| Epoch: 36/120, val_loss: 0.535, val_acc: 89.040, lr: 0.200000\n",
+      "| Epoch [ 37/120] Iter[352/352]\t\tLoss: 0.6234 Acc@1: 77.040%, time: 1.166| Epoch: 37/120, val_loss: 0.705, val_acc: 73.180, lr: 0.200000\n",
+      "| Epoch: 37/120, val_loss: 0.538, val_acc: 89.100, lr: 0.200000\n",
+      "| Epoch [ 38/120] Iter[352/352]\t\tLoss: 0.6248 Acc@1: 77.022%, time: 1.179| Epoch: 38/120, val_loss: 0.845, val_acc: 71.680, lr: 0.200000\n",
+      "| Epoch: 38/120, val_loss: 0.548, val_acc: 89.160, lr: 0.200000\n",
+      "| Epoch [ 39/120] Iter[352/352]\t\tLoss: 0.5596 Acc@1: 77.309%, time: 1.173| Epoch: 39/120, val_loss: 0.599, val_acc: 66.140, lr: 0.200000\n",
+      "| Epoch: 39/120, val_loss: 0.549, val_acc: 89.600, lr: 0.200000\n",
+      "| Epoch [ 40/120] Iter[352/352]\t\tLoss: 0.5635 Acc@1: 77.471%, time: 1.173| Epoch: 40/120, val_loss: 1.557, val_acc: 68.040, lr: 0.200000\n",
+      "| Epoch: 40/120, val_loss: 0.537, val_acc: 89.620, lr: 0.200000\n",
+      "| Epoch [ 41/120] Iter[352/352]\t\tLoss: 0.4870 Acc@1: 77.964%, time: 1.175| Epoch: 41/120, val_loss: 1.166, val_acc: 60.700, lr: 0.200000\n",
+      "| Epoch: 41/120, val_loss: 0.547, val_acc: 89.500, lr: 0.200000\n",
+      "| Epoch [ 42/120] Iter[352/352]\t\tLoss: 0.4301 Acc@1: 77.891%, time: 1.172| Epoch: 42/120, val_loss: 0.549, val_acc: 70.140, lr: 0.200000\n",
+      "| Epoch: 42/120, val_loss: 0.529, val_acc: 89.440, lr: 0.200000\n",
+      "| Epoch [ 43/120] Iter[352/352]\t\tLoss: 0.3525 Acc@1: 78.060%, time: 1.172| Epoch: 43/120, val_loss: 2.203, val_acc: 68.560, lr: 0.200000\n",
+      "| Epoch: 43/120, val_loss: 0.535, val_acc: 89.440, lr: 0.200000\n",
+      "| Epoch [ 44/120] Iter[352/352]\t\tLoss: 0.4111 Acc@1: 78.127%, time: 1.180| Epoch: 44/120, val_loss: 1.040, val_acc: 72.840, lr: 0.200000\n",
+      "| Epoch: 44/120, val_loss: 0.547, val_acc: 89.260, lr: 0.200000\n",
+      "| Epoch [ 45/120] Iter[352/352]\t\tLoss: 0.5244 Acc@1: 77.887%, time: 1.175| Epoch: 45/120, val_loss: 0.542, val_acc: 76.960, lr: 0.200000\n",
+      "| Epoch: 45/120, val_loss: 0.565, val_acc: 89.480, lr: 0.200000\n",
+      "| Epoch [ 46/120] Iter[352/352]\t\tLoss: 0.5644 Acc@1: 78.544%, time: 1.175| Epoch: 46/120, val_loss: 1.404, val_acc: 69.840, lr: 0.200000\n",
+      "| Epoch: 46/120, val_loss: 0.595, val_acc: 89.280, lr: 0.200000\n",
+      "| Epoch [ 47/120] Iter[352/352]\t\tLoss: 0.5870 Acc@1: 78.364%, time: 1.176| Epoch: 47/120, val_loss: 0.985, val_acc: 74.140, lr: 0.200000\n",
+      "| Epoch: 47/120, val_loss: 0.568, val_acc: 89.680, lr: 0.200000\n",
+      "| Epoch [ 48/120] Iter[352/352]\t\tLoss: 0.4845 Acc@1: 78.251%, time: 1.185| Epoch: 48/120, val_loss: 0.430, val_acc: 71.220, lr: 0.200000\n",
+      "| Epoch: 48/120, val_loss: 0.604, val_acc: 90.040, lr: 0.200000\n",
+      "| Epoch [ 49/120] Iter[352/352]\t\tLoss: 0.4621 Acc@1: 78.240%, time: 1.189| Epoch: 49/120, val_loss: 1.178, val_acc: 68.760, lr: 0.200000\n",
+      "| Epoch: 49/120, val_loss: 0.574, val_acc: 90.060, lr: 0.200000\n",
+      "| Epoch [ 50/120] Iter[352/352]\t\tLoss: 0.4173 Acc@1: 78.324%, time: 1.177| Epoch: 50/120, val_loss: 1.110, val_acc: 68.820, lr: 0.200000\n",
+      "| Epoch: 50/120, val_loss: 0.574, val_acc: 89.840, lr: 0.200000\n",
+      "| Epoch [ 51/120] Iter[352/352]\t\tLoss: 0.5124 Acc@1: 78.429%, time: 1.171| Epoch: 51/120, val_loss: 0.791, val_acc: 72.100, lr: 0.200000\n",
+      "| Epoch: 51/120, val_loss: 0.557, val_acc: 90.080, lr: 0.200000\n",
+      "| Epoch [ 52/120] Iter[352/352]\t\tLoss: 0.3174 Acc@1: 78.769%, time: 1.168| Epoch: 52/120, val_loss: 0.476, val_acc: 71.920, lr: 0.200000\n",
+      "| Epoch: 52/120, val_loss: 0.536, val_acc: 90.120, lr: 0.200000\n",
+      "| Epoch [ 53/120] Iter[352/352]\t\tLoss: 0.4958 Acc@1: 78.356%, time: 1.177| Epoch: 53/120, val_loss: 0.777, val_acc: 73.300, lr: 0.200000\n",
+      "| Epoch: 53/120, val_loss: 0.540, val_acc: 90.200, lr: 0.200000\n",
+      "| Epoch [ 54/120] Iter[352/352]\t\tLoss: 0.3997 Acc@1: 78.849%, time: 1.183| Epoch: 54/120, val_loss: 1.056, val_acc: 75.980, lr: 0.200000\n",
+      "| Epoch: 54/120, val_loss: 0.543, val_acc: 90.080, lr: 0.200000\n",
+      "| Epoch [ 55/120] Iter[352/352]\t\tLoss: 0.3966 Acc@1: 78.980%, time: 1.172| Epoch: 55/120, val_loss: 0.871, val_acc: 76.900, lr: 0.200000\n",
+      "| Epoch: 55/120, val_loss: 0.527, val_acc: 89.960, lr: 0.200000\n",
+      "| Epoch [ 56/120] Iter[352/352]\t\tLoss: 0.3950 Acc@1: 78.900%, time: 1.176| Epoch: 56/120, val_loss: 0.945, val_acc: 74.120, lr: 0.200000\n",
+      "| Epoch: 56/120, val_loss: 0.529, val_acc: 90.200, lr: 0.200000\n",
+      "| Epoch [ 57/120] Iter[352/352]\t\tLoss: 0.4378 Acc@1: 79.082%, time: 1.176| Epoch: 57/120, val_loss: 1.794, val_acc: 70.080, lr: 0.200000\n",
+      "| Epoch: 57/120, val_loss: 0.525, val_acc: 90.100, lr: 0.200000\n",
+      "| Epoch [ 58/120] Iter[352/352]\t\tLoss: 0.8051 Acc@1: 78.933%, time: 1.180| Epoch: 58/120, val_loss: 0.298, val_acc: 73.440, lr: 0.200000\n",
+      "| Epoch: 58/120, val_loss: 0.540, val_acc: 89.920, lr: 0.200000\n",
+      "| Epoch [ 59/120] Iter[352/352]\t\tLoss: 0.4176 Acc@1: 79.167%, time: 1.178| Epoch: 59/120, val_loss: 0.587, val_acc: 74.860, lr: 0.200000\n",
+      "| Epoch: 59/120, val_loss: 0.531, val_acc: 90.200, lr: 0.200000\n",
+      "| Epoch [ 60/120] Iter[352/352]\t\tLoss: 0.4358 Acc@1: 79.098%, time: 1.176| Epoch: 60/120, val_loss: 1.594, val_acc: 64.880, lr: 0.200000\n",
+      "| Epoch: 60/120, val_loss: 0.533, val_acc: 90.360, lr: 0.200000\n",
+      "| Epoch [ 61/120] Iter[352/352]\t\tLoss: 0.5189 Acc@1: 79.504%, time: 1.178| Epoch: 61/120, val_loss: 0.679, val_acc: 71.520, lr: 0.200000\n",
+      "| Epoch: 61/120, val_loss: 0.547, val_acc: 90.120, lr: 0.200000\n",
+      "| Epoch [ 62/120] Iter[352/352]\t\tLoss: 0.5648 Acc@1: 79.278%, time: 1.182| Epoch: 62/120, val_loss: 1.560, val_acc: 73.440, lr: 0.200000\n",
+      "| Epoch: 62/120, val_loss: 0.534, val_acc: 90.360, lr: 0.200000\n",
+      "| Epoch [ 63/120] Iter[352/352]\t\tLoss: 0.4782 Acc@1: 79.589%, time: 1.173| Epoch: 63/120, val_loss: 1.092, val_acc: 71.900, lr: 0.200000\n",
+      "| Epoch: 63/120, val_loss: 0.520, val_acc: 90.300, lr: 0.200000\n",
+      "| Epoch [ 64/120] Iter[352/352]\t\tLoss: 0.3329 Acc@1: 79.822%, time: 1.189| Epoch: 64/120, val_loss: 1.048, val_acc: 71.120, lr: 0.200000\n",
+      "| Epoch: 64/120, val_loss: 0.513, val_acc: 90.500, lr: 0.200000\n",
+      "| Epoch [ 65/120] Iter[352/352]\t\tLoss: 0.5376 Acc@1: 79.429%, time: 1.173| Epoch: 65/120, val_loss: 1.324, val_acc: 78.440, lr: 0.200000\n",
+      "| Epoch: 65/120, val_loss: 0.527, val_acc: 90.620, lr: 0.200000\n",
+      "| Epoch [ 66/120] Iter[352/352]\t\tLoss: 0.4513 Acc@1: 79.738%, time: 1.177| Epoch: 66/120, val_loss: 1.193, val_acc: 69.920, lr: 0.200000\n",
+      "| Epoch: 66/120, val_loss: 0.525, val_acc: 90.380, lr: 0.200000\n",
+      "| Epoch [ 67/120] Iter[352/352]\t\tLoss: 0.4331 Acc@1: 79.278%, time: 1.179| Epoch: 67/120, val_loss: 2.072, val_acc: 60.820, lr: 0.200000\n",
+      "| Epoch: 67/120, val_loss: 0.511, val_acc: 90.500, lr: 0.200000\n",
+      "| Epoch [ 68/120] Iter[352/352]\t\tLoss: 0.6960 Acc@1: 79.484%, time: 1.170| Epoch: 68/120, val_loss: 1.506, val_acc: 72.180, lr: 0.200000\n",
+      "| Epoch: 68/120, val_loss: 0.535, val_acc: 90.340, lr: 0.200000\n",
+      "| Epoch [ 69/120] Iter[352/352]\t\tLoss: 0.4762 Acc@1: 79.580%, time: 1.185| Epoch: 69/120, val_loss: 2.008, val_acc: 68.320, lr: 0.200000\n",
+      "| Epoch: 69/120, val_loss: 0.543, val_acc: 90.200, lr: 0.200000\n",
+      "| Epoch [ 70/120] Iter[352/352]\t\tLoss: 0.4891 Acc@1: 79.769%, time: 1.177| Epoch: 70/120, val_loss: 1.585, val_acc: 73.360, lr: 0.200000\n",
+      "| Epoch: 70/120, val_loss: 0.530, val_acc: 90.360, lr: 0.200000\n",
+      "| Epoch [ 71/120] Iter[352/352]\t\tLoss: 0.5251 Acc@1: 79.680%, time: 1.175| Epoch: 71/120, val_loss: 1.549, val_acc: 69.860, lr: 0.200000\n",
+      "| Epoch: 71/120, val_loss: 0.537, val_acc: 90.340, lr: 0.200000\n",
+      "| Epoch [ 72/120] Iter[352/352]\t\tLoss: 0.3641 Acc@1: 79.602%, time: 1.172| Epoch: 72/120, val_loss: 1.835, val_acc: 73.540, lr: 0.200000\n",
+      "| Epoch: 72/120, val_loss: 0.531, val_acc: 90.800, lr: 0.200000\n",
+      "| Epoch [ 73/120] Iter[352/352]\t\tLoss: 0.3378 Acc@1: 79.878%, time: 1.177| Epoch: 73/120, val_loss: 0.709, val_acc: 76.200, lr: 0.200000\n",
+      "| Epoch: 73/120, val_loss: 0.563, val_acc: 90.540, lr: 0.200000\n",
+      "| Epoch [ 74/120] Iter[207/352]\t\tLoss: 0.4397 Acc@1: 80.182%, time: 1.867"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "requests_with_retry encountered retryable exception: 500 Server Error: Internal Server Error for url: https://api.wandb.ai/files/xavierlopeze/Cifar_Experiment/3165tbv4/file_stream. args: ('https://api.wandb.ai/files/xavierlopeze/Cifar_Experiment/3165tbv4/file_stream',), kwargs: {'json': {'files': {'output.log': {'offset': 152, 'content': ['2020-05-12T07:03:08.536417 | Epoch [ 74/120] Iter[190/352]\\t\\tLoss: 0.4143 Acc@1: 80.144%, time: 1.850\\r']}}}}\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "| Epoch [ 74/120] Iter[210/352]\t\tLoss: 0.3724 Acc@1: 80.171%, time: 1.853"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "requests_with_retry encountered retryable exception: 500 Server Error: Internal Server Error for url: https://api.wandb.ai/files/xavierlopeze/Cifar_Experiment/3165tbv4/file_stream. args: ('https://api.wandb.ai/files/xavierlopeze/Cifar_Experiment/3165tbv4/file_stream',), kwargs: {'json': {'files': {'output.log': {'offset': 152, 'content': ['2020-05-12T07:03:08.536417 | Epoch [ 74/120] Iter[190/352]\\t\\tLoss: 0.4143 Acc@1: 80.144%, time: 1.850\\r']}}}}\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "| Epoch [ 74/120] Iter[212/352]\t\tLoss: 0.4912 Acc@1: 80.156%, time: 1.869"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "requests_with_retry encountered retryable exception: 500 Server Error: Internal Server Error for url: https://api.wandb.ai/files/xavierlopeze/Cifar_Experiment/3165tbv4/file_stream. args: ('https://api.wandb.ai/files/xavierlopeze/Cifar_Experiment/3165tbv4/file_stream',), kwargs: {'json': {'files': {'output.log': {'offset': 152, 'content': ['2020-05-12T07:03:08.536417 | Epoch [ 74/120] Iter[190/352]\\t\\tLoss: 0.4143 Acc@1: 80.144%, time: 1.850\\r']}}}}\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "| Epoch [ 74/120] Iter[218/352]\t\tLoss: 0.5194 Acc@1: 80.100%, time: 1.865"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "requests_with_retry encountered retryable exception: 500 Server Error: Internal Server Error for url: https://api.wandb.ai/files/xavierlopeze/Cifar_Experiment/3165tbv4/file_stream. args: ('https://api.wandb.ai/files/xavierlopeze/Cifar_Experiment/3165tbv4/file_stream',), kwargs: {'json': {'files': {'output.log': {'offset': 152, 'content': ['2020-05-12T07:03:08.536417 | Epoch [ 74/120] Iter[190/352]\\t\\tLoss: 0.4143 Acc@1: 80.144%, time: 1.850\\r']}}}}\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "| Epoch [ 74/120] Iter[352/352]\t\tLoss: 0.4643 Acc@1: 79.787%, time: 1.178| Epoch: 74/120, val_loss: 1.518, val_acc: 68.700, lr: 0.200000\n",
+      "| Epoch: 74/120, val_loss: 0.608, val_acc: 90.440, lr: 0.200000\n",
+      "| Epoch [ 75/120] Iter[352/352]\t\tLoss: 0.3654 Acc@1: 79.984%, time: 1.171| Epoch: 75/120, val_loss: 0.875, val_acc: 72.980, lr: 0.200000\n",
+      "| Epoch: 75/120, val_loss: 0.632, val_acc: 90.460, lr: 0.200000\n",
+      "| Epoch [ 76/120] Iter[352/352]\t\tLoss: 0.4473 Acc@1: 80.178%, time: 1.177| Epoch: 76/120, val_loss: 1.234, val_acc: 65.760, lr: 0.200000\n",
+      "| Epoch: 76/120, val_loss: 0.672, val_acc: 90.000, lr: 0.200000\n",
+      "| Epoch [ 77/120] Iter[352/352]\t\tLoss: 0.5459 Acc@1: 80.060%, time: 1.182| Epoch: 77/120, val_loss: 0.843, val_acc: 77.220, lr: 0.200000\n",
+      "| Epoch: 77/120, val_loss: 0.702, val_acc: 90.160, lr: 0.200000\n",
+      "| Epoch [ 78/120] Iter[352/352]\t\tLoss: 0.5129 Acc@1: 79.862%, time: 1.176| Epoch: 78/120, val_loss: 1.137, val_acc: 74.600, lr: 0.200000\n",
+      "| Epoch: 78/120, val_loss: 0.700, val_acc: 90.340, lr: 0.200000\n",
+      "| Epoch [ 79/120] Iter[352/352]\t\tLoss: 0.5198 Acc@1: 80.113%, time: 1.180| Epoch: 79/120, val_loss: 0.762, val_acc: 69.960, lr: 0.200000\n",
+      "| Epoch: 79/120, val_loss: 0.657, val_acc: 90.260, lr: 0.200000\n",
+      "| Epoch [ 80/120] Iter[352/352]\t\tLoss: 0.3937 Acc@1: 80.271%, time: 1.186| Epoch: 80/120, val_loss: 0.926, val_acc: 73.260, lr: 0.200000\n",
+      "| Epoch: 80/120, val_loss: 0.654, val_acc: 90.320, lr: 0.200000\n",
+      "| Epoch [ 81/120] Iter[352/352]\t\tLoss: 0.3713 Acc@1: 83.880%, time: 1.181| Epoch: 81/120, val_loss: 1.184, val_acc: 76.300, lr: 0.020000\n",
+      "| Epoch: 81/120, val_loss: 0.708, val_acc: 89.980, lr: 0.020000\n",
+      "| Epoch [ 82/120] Iter[352/352]\t\tLoss: 0.2604 Acc@1: 84.887%, time: 1.177| Epoch: 82/120, val_loss: 1.389, val_acc: 79.360, lr: 0.020000\n",
+      "| Epoch: 82/120, val_loss: 0.792, val_acc: 89.480, lr: 0.020000\n",
+      "| Epoch [ 83/120] Iter[352/352]\t\tLoss: 0.2802 Acc@1: 85.582%, time: 1.179| Epoch: 83/120, val_loss: 2.070, val_acc: 76.080, lr: 0.020000\n",
+      "| Epoch: 83/120, val_loss: 0.901, val_acc: 89.080, lr: 0.020000\n",
+      "| Epoch [ 84/120] Iter[352/352]\t\tLoss: 0.2033 Acc@1: 85.971%, time: 1.172| Epoch: 84/120, val_loss: 1.453, val_acc: 77.240, lr: 0.020000\n",
+      "| Epoch: 84/120, val_loss: 0.952, val_acc: 88.780, lr: 0.020000\n",
+      "| Epoch [ 85/120] Iter[352/352]\t\tLoss: 0.3188 Acc@1: 86.691%, time: 1.170| Epoch: 85/120, val_loss: 1.973, val_acc: 74.940, lr: 0.020000\n",
+      "| Epoch: 85/120, val_loss: 1.081, val_acc: 87.960, lr: 0.020000\n",
+      "| Epoch [ 86/120] Iter[352/352]\t\tLoss: 0.2710 Acc@1: 86.902%, time: 1.175| Epoch: 86/120, val_loss: 1.547, val_acc: 78.300, lr: 0.020000\n",
+      "| Epoch: 86/120, val_loss: 1.063, val_acc: 87.840, lr: 0.020000\n",
+      "| Epoch [ 87/120] Iter[352/352]\t\tLoss: 0.2505 Acc@1: 87.580%, time: 1.182| Epoch: 87/120, val_loss: 1.683, val_acc: 77.880, lr: 0.020000\n",
+      "| Epoch: 87/120, val_loss: 1.172, val_acc: 87.240, lr: 0.020000\n",
+      "| Epoch [ 88/120] Iter[352/352]\t\tLoss: 0.2521 Acc@1: 87.958%, time: 1.171| Epoch: 88/120, val_loss: 2.544, val_acc: 74.340, lr: 0.020000\n",
+      "| Epoch: 88/120, val_loss: 1.352, val_acc: 86.740, lr: 0.020000\n",
+      "| Epoch [ 89/120] Iter[352/352]\t\tLoss: 0.2110 Acc@1: 88.369%, time: 1.171| Epoch: 89/120, val_loss: 2.850, val_acc: 75.700, lr: 0.020000\n",
+      "| Epoch: 89/120, val_loss: 1.401, val_acc: 86.440, lr: 0.020000\n",
+      "| Epoch [ 90/120] Iter[352/352]\t\tLoss: 0.2113 Acc@1: 88.782%, time: 1.173| Epoch: 90/120, val_loss: 1.849, val_acc: 75.520, lr: 0.020000\n",
+      "| Epoch: 90/120, val_loss: 1.413, val_acc: 86.220, lr: 0.020000\n",
+      "| Epoch [ 91/120] Iter[352/352]\t\tLoss: 0.2174 Acc@1: 89.440%, time: 1.180| Epoch: 91/120, val_loss: 2.040, val_acc: 72.780, lr: 0.020000\n",
+      "| Epoch: 91/120, val_loss: 1.552, val_acc: 85.700, lr: 0.020000\n",
+      "| Epoch [ 92/120] Iter[352/352]\t\tLoss: 0.1566 Acc@1: 89.849%, time: 1.180| Epoch: 92/120, val_loss: 1.889, val_acc: 73.280, lr: 0.020000\n",
+      "| Epoch: 92/120, val_loss: 1.538, val_acc: 85.600, lr: 0.020000\n",
+      "| Epoch [ 93/120] Iter[352/352]\t\tLoss: 0.1886 Acc@1: 90.347%, time: 1.180| Epoch: 93/120, val_loss: 1.677, val_acc: 74.880, lr: 0.020000\n",
+      "| Epoch: 93/120, val_loss: 1.747, val_acc: 85.040, lr: 0.020000\n",
+      "| Epoch [ 94/120] Iter[352/352]\t\tLoss: 0.1838 Acc@1: 90.842%, time: 1.177| Epoch: 94/120, val_loss: 1.484, val_acc: 74.740, lr: 0.020000\n",
+      "| Epoch: 94/120, val_loss: 1.903, val_acc: 84.760, lr: 0.020000\n",
+      "| Epoch [ 95/120] Iter[352/352]\t\tLoss: 0.1341 Acc@1: 91.407%, time: 1.173| Epoch: 95/120, val_loss: 2.381, val_acc: 67.680, lr: 0.020000\n",
+      "| Epoch: 95/120, val_loss: 2.143, val_acc: 83.860, lr: 0.020000\n",
+      "| Epoch [ 96/120] Iter[352/352]\t\tLoss: 0.1806 Acc@1: 91.889%, time: 1.177| Epoch: 96/120, val_loss: 2.619, val_acc: 71.700, lr: 0.020000\n",
+      "| Epoch: 96/120, val_loss: 2.264, val_acc: 83.520, lr: 0.020000\n",
+      "| Epoch [ 97/120] Iter[352/352]\t\tLoss: 0.1071 Acc@1: 92.149%, time: 1.173| Epoch: 97/120, val_loss: 2.914, val_acc: 70.140, lr: 0.020000\n",
+      "| Epoch: 97/120, val_loss: 2.326, val_acc: 83.100, lr: 0.020000\n",
+      "| Epoch [ 98/120] Iter[352/352]\t\tLoss: 0.1820 Acc@1: 92.458%, time: 1.181| Epoch: 98/120, val_loss: 4.691, val_acc: 65.480, lr: 0.020000\n",
+      "| Epoch: 98/120, val_loss: 2.270, val_acc: 82.920, lr: 0.020000\n",
+      "| Epoch [ 99/120] Iter[352/352]\t\tLoss: 0.1412 Acc@1: 92.860%, time: 1.182| Epoch: 99/120, val_loss: 1.452, val_acc: 72.300, lr: 0.020000\n",
+      "| Epoch: 99/120, val_loss: 2.435, val_acc: 82.620, lr: 0.020000\n",
+      "| Epoch [100/120] Iter[352/352]\t\tLoss: 0.1495 Acc@1: 93.458%, time: 1.178| Epoch: 100/120, val_loss: 1.562, val_acc: 69.900, lr: 0.020000\n",
+      "| Epoch: 100/120, val_loss: 2.463, val_acc: 82.340, lr: 0.020000\n",
+      "| Epoch [101/120] Iter[352/352]\t\tLoss: 0.1872 Acc@1: 94.040%, time: 1.171| Epoch: 101/120, val_loss: 2.528, val_acc: 69.740, lr: 0.020000\n",
+      "| Epoch: 101/120, val_loss: 2.406, val_acc: 82.560, lr: 0.020000\n",
+      "| Epoch [102/120] Iter[352/352]\t\tLoss: 0.0592 Acc@1: 94.391%, time: 1.169| Epoch: 102/120, val_loss: 3.651, val_acc: 70.240, lr: 0.020000\n",
+      "| Epoch: 102/120, val_loss: 2.722, val_acc: 81.800, lr: 0.020000\n",
+      "| Epoch [103/120] Iter[352/352]\t\tLoss: 0.3371 Acc@1: 94.578%, time: 1.169| Epoch: 103/120, val_loss: 4.552, val_acc: 70.840, lr: 0.020000\n",
+      "| Epoch: 103/120, val_loss: 2.790, val_acc: 82.140, lr: 0.020000\n",
+      "| Epoch [104/120] Iter[352/352]\t\tLoss: 0.1441 Acc@1: 95.020%, time: 1.179| Epoch: 104/120, val_loss: 3.144, val_acc: 69.500, lr: 0.020000\n",
+      "| Epoch: 104/120, val_loss: 2.895, val_acc: 82.060, lr: 0.020000\n",
+      "| Epoch [105/120] Iter[352/352]\t\tLoss: 0.1617 Acc@1: 95.144%, time: 1.170| Epoch: 105/120, val_loss: 2.954, val_acc: 67.560, lr: 0.020000\n",
+      "| Epoch: 105/120, val_loss: 2.813, val_acc: 82.180, lr: 0.020000\n",
+      "| Epoch [106/120] Iter[161/352]\t\tLoss: 0.2388 Acc@1: 95.851%, time: 1.844"
+     ]
+    }
+   ],
+   "source": [
+    "init = True\n",
+    "best_acc = 0\n",
+    "use_mentor = False\n",
+    "for epoch in range(1, 1 + config.num_epochs):\n",
+    "    train(epoch)\n",
+    "    # Student validation\n",
+    "    std_results = valid(epoch, net)\n",
+    "    record.write(std_results + '\\n')\n",
+    "    print(std_results)\n",
+    "    # Teacher validation\n",
+    "    tch_results = valid(epoch, tch_net)\n",
+    "    record.write(tch_results + '\\n')\n",
+    "    record.flush()\n",
+    "    print(tch_results)\n",
+    "\n",
+    "    save_weights(epoch)\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 11,
+   "metadata": {
+    "colab": {
+     "base_uri": "https://localhost:8080/",
+     "height": 545
+    },
+    "colab_type": "code",
+    "id": "2SMIjoGjmj_B",
+    "outputId": "44b290a8-3844-4aa4-d813-243d85bf8426"
+   },
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "\n",
+      "Testing model\n"
+     ]
+    },
+    {
+     "ename": "OSError",
+     "evalue": "ignored",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
+      "\u001b[0;32m<ipython-input-11-eef504839d31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrive_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/checkpoint/%s.pth.tar'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
+      "\u001b[0;32m<ipython-input-8-1c6144fbac3b>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    836\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
+      "\u001b[0;31mOSError\u001b[0m: Caught OSError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/content/drive/My Drive/Colab_Notebooks/git/PFM_Noisy_Labels/MLNT_cifar/dataloader.py\", line 61, in __getitem__\n    image = Image.open(img_path).convert('RGB')\n  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 2809, in open\n    fp = builtins.open(filename, \"rb\")\nOSError: [Errno 5] Input/output error: '/content/drive/My Drive/Colab_Notebooks/git/PFM_Noisy_Labels/MLNT_cifar/data/train/airplane/4629.png'\n"
+     ]
+    }
+   ],
+   "source": [
+    "print('\\nTesting model')\n",
+    "checkpoint = torch.load(config.drive_dir + '/checkpoint/%s.pth.tar' % config.id)\n",
+    "test_net.load_state_dict(checkpoint['state_dict'])\n",
+    "test()\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 12,
+   "metadata": {
+    "colab": {
+     "base_uri": "https://localhost:8080/",
+     "height": 68
+    },
+    "colab_type": "code",
+    "id": "aprjXZNfmr-v",
+    "outputId": "d1edf60a-15c3-4b32-9fb6-e45c031cfb1d"
+   },
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Load Student\n",
+      "Load Teacher\n"
+     ]
+    },
+    {
+     "data": {
+      "text/plain": [
+       "<All keys matched successfully>"
+      ]
+     },
+     "execution_count": 12,
+     "metadata": {
+      "tags": []
+     },
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "#This code is useful when the code has crashed without getting to the maximum of epochs (due to google collab session termination)\n",
+    "#essentially we reload weights and following continue the training process\n",
+    "\n",
+    "EPOCH = 104 #Input the last epoch computed\n",
+    "\n",
+    "print('Load Student')\n",
+    "checkpoint = torch.load(config.drive_dir + '/checkpoint/' + config.id + '_student_' + str(EPOCH) + '.pth.tar' )\n",
+    "net.load_state_dict(checkpoint['state_dict'])\n",
+    "\n",
+    "print('Load Teacher')\n",
+    "\n",
+    "checkpoint = torch.load(config.drive_dir + '/checkpoint/' + config.id + '_teacher_' + str(EPOCH) + '.pth.tar' )\n",
+    "tch_net.load_state_dict(checkpoint['state_dict'])"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 13,
+   "metadata": {
+    "colab": {
+     "base_uri": "https://localhost:8080/",
+     "height": 770
+    },
+    "colab_type": "code",
+    "id": "W84wMN7xpNLP",
+    "outputId": "27b24c1d-e52f-453b-e51e-3861ed1ac82d"
+   },
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
+      "\tadd_(Number alpha, Tensor other)\n",
+      "Consider using one of the following signatures instead:\n",
+      "\tadd_(Tensor other, *, Number alpha)\n",
+      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2247: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
+      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "| Epoch: 104/120, val_loss: 0.666, val_acc: 79.140, lr: 0.020000\n",
+      "| Epoch: 104/120, val_loss: 2.156, val_acc: 83.000, lr: 0.020000\n",
+      "| Epoch [105/120] Iter[352/352]\t\tLoss: 0.2513 Acc@1: 82.360%, time: 1.204| Epoch: 105/120, val_loss: 0.690, val_acc: 80.420, lr: 0.020000\n",
+      "| Epoch: 105/120, val_loss: 1.681, val_acc: 85.020, lr: 0.020000\n",
+      "| Epoch [106/120] Iter[352/352]\t\tLoss: 0.3060 Acc@1: 82.836%, time: 1.207| Epoch: 106/120, val_loss: 0.862, val_acc: 80.140, lr: 0.020000\n",
+      "| Epoch: 106/120, val_loss: 1.427, val_acc: 86.380, lr: 0.020000\n",
+      "| Epoch [107/120] Iter[352/352]\t\tLoss: 0.2364 Acc@1: 83.100%, time: 1.199| Epoch: 107/120, val_loss: 0.858, val_acc: 78.620, lr: 0.020000\n",
+      "| Epoch: 107/120, val_loss: 1.221, val_acc: 87.540, lr: 0.020000\n",
+      "| Epoch [108/120] Iter[352/352]\t\tLoss: 0.2999 Acc@1: 83.402%, time: 1.210| Epoch: 108/120, val_loss: 0.601, val_acc: 81.380, lr: 0.020000\n",
+      "| Epoch: 108/120, val_loss: 1.050, val_acc: 88.060, lr: 0.020000\n",
+      "| Epoch [109/120] Iter[352/352]\t\tLoss: 0.2565 Acc@1: 83.660%, time: 1.203| Epoch: 109/120, val_loss: 0.871, val_acc: 81.940, lr: 0.020000\n",
+      "| Epoch: 109/120, val_loss: 0.949, val_acc: 88.420, lr: 0.020000\n",
+      "| Epoch [110/120] Iter[352/352]\t\tLoss: 0.2172 Acc@1: 84.031%, time: 1.209| Epoch: 110/120, val_loss: 0.764, val_acc: 79.680, lr: 0.020000\n",
+      "| Epoch: 110/120, val_loss: 0.886, val_acc: 88.600, lr: 0.020000\n",
+      "| Epoch [111/120] Iter[352/352]\t\tLoss: 0.2508 Acc@1: 84.204%, time: 1.213| Epoch: 111/120, val_loss: 0.922, val_acc: 79.580, lr: 0.020000\n",
+      "| Epoch: 111/120, val_loss: 0.813, val_acc: 88.480, lr: 0.020000\n",
+      "| Epoch [112/120] Iter[352/352]\t\tLoss: 0.2199 Acc@1: 84.527%, time: 1.205| Epoch: 112/120, val_loss: 0.752, val_acc: 77.480, lr: 0.020000\n",
+      "| Epoch: 112/120, val_loss: 0.806, val_acc: 88.360, lr: 0.020000\n",
+      "| Epoch [113/120] Iter[352/352]\t\tLoss: 0.2824 Acc@1: 84.962%, time: 1.211| Epoch: 113/120, val_loss: 1.139, val_acc: 77.280, lr: 0.020000\n",
+      "| Epoch: 113/120, val_loss: 0.907, val_acc: 87.900, lr: 0.020000\n",
+      "| Epoch [114/120] Iter[352/352]\t\tLoss: 0.2545 Acc@1: 85.307%, time: 1.203| Epoch: 114/120, val_loss: 1.257, val_acc: 75.320, lr: 0.020000\n",
+      "| Epoch: 114/120, val_loss: 0.975, val_acc: 87.140, lr: 0.020000\n",
+      "| Epoch [115/120] Iter[352/352]\t\tLoss: 0.2728 Acc@1: 85.904%, time: 1.211| Epoch: 115/120, val_loss: 1.875, val_acc: 75.660, lr: 0.020000\n",
+      "| Epoch: 115/120, val_loss: 1.018, val_acc: 86.740, lr: 0.020000\n",
+      "| Epoch [116/120] Iter[352/352]\t\tLoss: 0.2936 Acc@1: 86.338%, time: 1.205| Epoch: 116/120, val_loss: 1.438, val_acc: 75.880, lr: 0.020000\n",
+      "| Epoch: 116/120, val_loss: 1.047, val_acc: 86.360, lr: 0.020000\n",
+      "| Epoch [117/120] Iter[352/352]\t\tLoss: 0.1620 Acc@1: 86.909%, time: 1.201| Epoch: 117/120, val_loss: 1.213, val_acc: 74.360, lr: 0.020000\n",
+      "| Epoch: 117/120, val_loss: 1.035, val_acc: 85.640, lr: 0.020000\n",
+      "| Epoch [118/120] Iter[352/352]\t\tLoss: 0.2754 Acc@1: 87.571%, time: 1.208| Epoch: 118/120, val_loss: 0.894, val_acc: 77.160, lr: 0.020000\n",
+      "| Epoch: 118/120, val_loss: 1.099, val_acc: 84.820, lr: 0.020000\n",
+      "| Epoch [119/120] Iter[352/352]\t\tLoss: 0.2402 Acc@1: 88.236%, time: 1.204| Epoch: 119/120, val_loss: 1.020, val_acc: 74.840, lr: 0.020000\n",
+      "| Epoch: 119/120, val_loss: 1.117, val_acc: 83.920, lr: 0.020000\n",
+      "| Epoch [120/120] Iter[352/352]\t\tLoss: 0.2367 Acc@1: 88.904%, time: 1.199| Epoch: 120/120, val_loss: 1.798, val_acc: 72.100, lr: 0.020000\n",
+      "| Epoch: 120/120, val_loss: 1.247, val_acc: 84.080, lr: 0.020000\n",
+      "\n",
+      "Testing model\n",
+      "| test_loss: 0.886, test_acc: 88.600\n"
+     ]
+    }
+   ],
+   "source": [
+    "init = False\n",
+    "best_acc = 0\n",
+    "for epoch in range(EPOCH, 1 + config.num_epochs):\n",
+    "    train(epoch)\n",
+    "    # Student validation\n",
+    "    std_results = valid(epoch, net)\n",
+    "    record.write(std_results + '\\n')\n",
+    "    print(std_results)\n",
+    "    # Teacher validation\n",
+    "    tch_results = valid(epoch, tch_net)\n",
+    "    record.write(tch_results + '\\n')\n",
+    "    record.flush()\n",
+    "    print(tch_results)\n",
+    "\n",
+    "    save_weights(epoch)\n",
+    "\n",
+    "print('\\nTesting model')\n",
+    "checkpoint = torch.load(config.drive_dir + '/checkpoint/%s.pth.tar' % config.id)\n",
+    "test_net.load_state_dict(checkpoint['state_dict'])\n",
+    "test()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab": {},
+    "colab_type": "code",
+    "id": "AFwEFJ3XUqDO"
+   },
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab": {},
+    "colab_type": "code",
+    "id": "v0KxssiuVjCi"
+   },
+   "outputs": [],
+   "source": [
+    "# DISCARDING SAMPLES"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab": {},
+    "colab_type": "code",
+    "id": "R0khwD7UUqQw"
+   },
+   "outputs": [],
+   "source": [
+    "# RELOAD NETWORKS\n",
+    "EPOCH = 120 #Input the last epoch computed\n",
+    "\n",
+    "print('Load Student')\n",
+    "checkpoint = torch.load(config.drive_dir + '/checkpoint/' + config.id + '_student_' + str(EPOCH) + '.pth.tar' )\n",
+    "net.load_state_dict(checkpoint['state_dict'])\n",
+    "\n",
+    "print('Load Teacher')\n",
+    "\n",
+    "checkpoint = torch.load(config.drive_dir + '/checkpoint/' + config.id + '_teacher_' + str(EPOCH) + '.pth.tar' )\n",
+    "tch_net.load_state_dict(checkpoint['state_dict'])\n",
+    "\n",
+    "print('\\nLoad Test')\n",
+    "checkpoint = torch.load(config.drive_dir + '/checkpoint/%s.pth.tar' % config.id)\n",
+    "test_net.load_state_dict(checkpoint['state_dict'])"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab": {},
+    "colab_type": "code",
+    "id": "SRzBS_nnVEnH"
+   },
+   "outputs": [],
+   "source": [
+    "import pandas\n",
+    "def get_confidence_training(dataloader):\n",
+    "  correct = 0\n",
+    "  total = 0\n",
+    "\n",
+    "  target_list = []\n",
+    "  pred        = []\n",
+    "  confidence  = []\n",
+    "\n",
+    "  for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
+    "      if use_cuda:\n",
+    "          inputs, targets = inputs.cuda(), targets.cuda()\n",
+    "      with torch.no_grad():\n",
+    "          outputs = test_net(inputs)\n",
+    "          loss = criterion(outputs, targets)  \n",
+    "          _, predicted = torch.max(outputs.data, 1)\n",
+    "          total += targets.size(0)\n",
+    "          correct += predicted.eq(targets.data).cpu().sum()\n",
+    "          \n",
+    "          conf = float(torch.max(outputs.softmax(dim = 1)))\n",
+    "          targ = int(targets.data)\n",
+    "          pd = int(predicted)\n",
+    "\n",
+    "          confidence.append(conf)\n",
+    "          target_list.append(targ)\n",
+    "          pred.append(pd)\n",
+    "\n",
+    "  # Grab dataframe\n",
+    "  df = pandas.DataFrame()\n",
+    "  df[\"target_list\"] = target_list\n",
+    "  df[\"pred\"] = pred\n",
+    "  df[\"confidence\"] = confidence\n",
+    "\n",
+    "  return(df)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab": {},
+    "colab_type": "code",
+    "id": "rT1TFAbAUqat"
+   },
+   "outputs": [],
+   "source": [
+    "#Those parameters are key for discarding\n",
+    "pre_batch = config.batch_size\n",
+    "config.batch_size = 1\n",
+    "config.shuffle = False\n",
+    "\n",
+    "# Get the dataloader with no shuffle\n",
+    "loader = dataloader.KeyDataLoader()\n",
+    "train_noshuffle_loader, _, _ = loader.run()\n",
+    "\n",
+    "\n",
+    "df = get_confidence_training(train_noshuffle_loader)\n",
+    "\n",
+    "#get list of directories\n",
+    "train_df = pandas.read_csv(config.data_dir+config.train_dir, header=None)\n",
+    "train_df.columns = [\"dir\"]\n",
+    "\n",
+    "#Merge directories with confidence\n",
+    "DF = df.merge(train_df,how = 'left', left_index = True, right_index = True)\n",
+    "\n",
+    "#filter only training samples that have a high enough confidence by the softmax\n",
+    "tau = 0.01\n",
+    "df_filtered = DF[DF.confidence > tau]\n",
+    "df_filtered\n",
+    "\n",
+    "#Export the filtered train dataset\n",
+    "df_filtered.to_csv(config.data_dir +'clean_train_key_list_iter2' + config.id + '.txt', header = None, index=False)\n",
+    "config.train_dir = 'clean_train_key_list_iter2' + config.id + '.txt'\n",
+    "\n",
+    "#Reset shuffling for next iteration\n",
+    "config.shuffle = True\n",
+    "config.batch_size = pre_batch\n",
+    "\n",
+    "# Get the filtered dataset\n",
+    "loader = dataloader.KeyDataLoader()\n",
+    "train_loader, _, _ = loader.run()\n",
+    "\n",
+    "#Initialize mentor\n",
+    "mentor_net = get_model()\n",
+    "if use_cuda:\n",
+    "    mentor_net.cuda()\n",
+    "\n",
+    "#Get the weights for the mentor\n",
+    "print('\\nLoad Mentor')\n",
+    "checkpoint = torch.load(config.drive_dir + '/checkpoint/%s.pth.tar' % config.id)\n",
+    "mentor_net.load_state_dict(checkpoint['state_dict'])\n",
+    "\n",
+    "#set mentor parameter on the train funciton\n",
+    "use_mentor = True"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab": {},
+    "colab_type": "code",
+    "id": "1BXqDeVmUqyB"
+   },
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab": {},
+    "colab_type": "code",
+    "id": "VAMbXSV3Uq1E"
+   },
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab": {},
+    "colab_type": "code",
+    "id": "AWUwbhfNUqWk"
+   },
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab": {},
+    "colab_type": "code",
+    "id": "xhqnhZ2YUqUE"
+   },
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text",
+    "id": "oyyFzY5r6yTV"
+   },
+   "source": []
+  }
+ ],
+ "metadata": {
+  "accelerator": "GPU",
+  "colab": {
+   "collapsed_sections": [],
+   "machine_shape": "hm",
+   "name": "main.ipynb",
+   "provenance": []
+  },
+  "kernelspec": {
+   "display_name": "Python 3",
+   "language": "python",
+   "name": "python3"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 1
+}
diff --git a/MLNT_cifar/models/__init__.py b/MLNT_cifar/models/__init__.py
index 1577362..75d4c6b 100644
--- a/MLNT_cifar/models/__init__.py
+++ b/MLNT_cifar/models/__init__.py
@@ -1,2 +1,3 @@
 from .resnet import *
+from .adaptive_vae import *
 # from .densenet import *
diff --git a/MLNT_cifar/models/__pycache__/resnet.cpython-36.pyc b/MLNT_cifar/models/__pycache__/resnet.cpython-36.pyc
index 1efa904..00c85d8 100644
Binary files a/MLNT_cifar/models/__pycache__/resnet.cpython-36.pyc and b/MLNT_cifar/models/__pycache__/resnet.cpython-36.pyc differ
